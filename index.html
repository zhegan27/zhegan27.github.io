<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 2.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
	<meta name="description" content="Principal Researcher, Microsoft Azure AI" />
	<meta name="keywords" content="Zhe Gan, Deep Learning, Machine Learning, Natural Language Processing, Computer Vision, Microsoft, Duke University, Peking University" />
	<meta name="Zhe Gan" content="Research" />
	<link rel="stylesheet" type="text/css" href="Zhe.css" title="Basic Profile" media="all" />
	<title>Zhe Gan</title>
</head>

<body>


	<div id="sidebar">
		<a href="index.html"><img src="images/Zhe_new.jpg"  height="180" alt="Sample logotype" /></a>
		<h1><a style="text-decoration: none" href="index.html">Zhe Gan</a></h1>
		<!-- <p class="slogan">everyone has a story to tell</p> -->
		
		<ul>
			<!--<li><a href="#">Page one</a><br />The front page...</li>-->
			<li><a href="Paper.html">Publications</a></li>
			<li><a href="https://scholar.google.com/citations?user=E64XWyMAAAAJ&hl=en">Google Scholar</a></li>
			<li><a href="https://github.com/zhegan27">GitHub</a></li>
			<li><a href="https://www.linkedin.com/pub/zhe-gan/78/29a/a22">LinkedIn</a></li>
			<li><a href="CV_Zhe.pdf">CV</a></li>
			<p>
			<font size="2">Principal Researcher<br>
			Microsoft Azure AI<br>
			Redmond, WA 98052<br>
			Email: zhe.gan@microsoft.com <br></font>
		</ul>
	</div>
	
	<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-35321140-3', ‘zhegan.github.io');
  ga('send', 'pageview');

</script>
	
	<!--style="font-family: Calibri; color: blue; text-decoration: underline;"-->

	<div id="content">
		<!--<h2>What Starts Here Changes The World!</h2>-->
		<p>
		<p>
		<br>
		
		<p> I am a Principal Researcher at Microsoft Azure AI, primarily working on building large-scale multimodal foundation models, under <a style="color: #CC5500;" href="https://www.microsoft.com/en-us/research/project/project-florence-vl/">Project Florence-VL</a>. I received my Ph.D. degree from <a style="color: #CC5500;" href="http://www.duke.edu/">Duke University</a> in Spring 2018. Before that, I received my Master's and B.Sc. from <a style="color: #CC5500;" href="http://www.pku.edu.cn/">Peking University</a> in 2013 and 2010, respectively. My Ph.D. advisor is <a style="color: #CC5500;" href="http://people.ee.duke.edu/~lcarin">Lawrence Carin</a>. I can be reached at zhe.gan@microsoft.com.


		<p> I am serving (or, has served) as an Area Chair for <a style="color: #CC5500;" href="https://nips.cc/Conferences/2022">NeurIPS 2022/2021/2020/2019</a>, <a style="color: #CC5500;" href="https://icml.cc/Conferences/2022">ICML 2022/2021</a>, <a style="color: #CC5500;" href="https://iclr.cc/">ICLR 2023/2021</a>, <a style="color: #CC5500;" href="https://cvpr2023.thecvf.com/">CVPR 2023</a>, <a style="color: #CC5500;" href="https://eccv2022.ecva.net/">ECCV 2022</a>, <a style="color: #CC5500;" href="https://2022.aclweb.org/">ACL 2022/2021</a>, <a style="color: #CC5500;" href="https://2022.naacl.org/">NAACL 2022</a>, <a style="color: #CC5500;" href="https://2022.emnlp.org/">EMNLP 2022</a>, <a style="color: #CC5500;" href="https://aaai.org/Conferences/AAAI/aaai.php">AAAI 2023/2022</a>, and a Senior Program Committee (SPC) member for <a style="color: #CC5500;" href="https://aaai.org/Conferences/AAAI/aaai.php">AAAI 2021/2020</a>, and received <a style="color: #CC5500;" href="https://aaai.org/Awards/conference.php">AAAI-20 Outstanding SPC Award</a>. Together with my co-authors, I have also received the Best Student Paper Honorable Mention Awards at CVPR 2021 and WACV 2021, respectively.
		
		<p>
		<br/>
		
		<h3>Research Highlights:  </h3>
		<ul>

		<p> 
		<li>
		[2022/9] 3 papers accepted to NeurIPS 2022: (1) <a style="color: #CC5500;" href="https://arxiv.org/abs/2207.09814">NUWA Infinity</a>, our new multimodal generative model for image synthesis; (2) <a style="color: #CC5500;" href="https://arxiv.org/pdf/2206.07643.pdf">FIBER</a>, our new VLP model that provides a unified solution for both VL understanding and localization tasks; and (3) <a style="color: #CC5500;" href="https://arxiv.org/pdf/2204.09222.pdf">K-Lite</a>, which explores how to enhance UniCL and GLIP models with external knowledge.  
		</li>

		<p> 
		<li>
		[2022/9] <a style="color: #CC5500;" href="https://arxiv.org/pdf/2111.12085.pdf">UniTAB</a> is accepted as an Oral paper at ECCV 2022. Following the line of work such as Pix2seq, Pix2seqV2, OFA, and Unified-IO, we propose a simple unified seq2seq learning framework that can output sequences with mixed text and box tokens.
		</li>

		<p> 
		<li>
		[2022/7] <a style="color: #CC5500;" href="https://arxiv.org/abs/2207.09814">NUWA Infinity</a> is our new multimodal generative model that is able to generate high-quality images and videos from given text or image input. We can generate images with resolution up to 38912 × 2048 pixels. Check our project website <a style="color: #CC5500;" href="https://nuwa-infinity.microsoft.com">here</a>.
		</li>

		<p> 
		<li>
		[2022/6] We held a tutorial on recent advances on vision-language pre-training at CVPR 2022. All our slides are available at our <a style="color: #CC5500;" href="https://vlp-tutorial.github.io/">tutorial website</a> now.
		</li>

		<p> 
		<li>
		[2022/6] <a style="color: #CC5500;" href="https://arxiv.org/pdf/2205.14100.pdf">Florence-GIT</a> is our new multimodal generative foundation model, where we have trained a simple image-to-text transformer on 800M image-text pairs. GIT achieves new sota across 12 image/video captioning and QA tasks, including the first human-parity on TextCaps. GIT achieves an accuracy of 88.79% on ImageNet-1k using a generative scheme. GIT can recognize logos, landmarks, characters, etc.
		</li>

		<p> 
		<li>
		[2022/3] 4 papers accepted to CVPR 2022, including (i) METER, an end-to-end transformer-based VLP model; (ii) LEMON, scaling up VLP for image captioning; (iii) SwinBERT, video-Swin-based video captioning model; and (iv) ViTCAP, ViT-based image captioning model. 
		</li>

		<p> 
		<li>
		[2021/10] 3 papers accepted to the main conference track of NeurIPS 2021, including Sparse ViT, Elastic LTH, and GAN lottery tickets; and 2 papers accepted to the datasets and benchmarks track of NeurIPS 2021, including AdvGLUE and VALUE. 
		</li>

		<p> 
		<li>
		[2021/10] During the summer, we have hosted a special Vision-Language Talk Series. With 11 invited speakers from both academia and industry, we have covered diverse topics ranging from image captioning, VQA, multimodal pre-training (ALIGN, MDETR), grounded visual generation, zero-shot object detection (ViLD), video-language understanding (MERLOT), to self-supervised learning (MoCo-v3). Want to know more?
		Please check the <a style="color: #CC5500;" href="https://www.youtube.com/playlist?list=PLD7HFcN7LXReRU4tDhN4VPTl4BpyTKAco">YouTube playlist</a> and <a style="color: #CC5500;" href="https://www.microsoft.com/en-us/research/videos/vision-language-summer-talk-series/">MSR video series</a>.
		</li>

		<p> 
		<li>
		[2021/09] We all know GPT-3 is a strong few-shot learner for NLP problems, but can it also benefit multimodal tasks? In <a style="color: #CC5500;" href="https://arxiv.org/pdf/2109.05014.pdf">this new work</a>, we provide an empirical study of GPT-3 for knowledge-based VQA, and show that prompting GPT-3 via the use of image captions with only 16 examples surpasses supervised sota by an absolute +8.6 points on the OK-VQA dataset (from 39.4 to 48.0). 
		</li>

		<p> 
		<li>
		[2021/07] Our Adversarial VQA work is accepted by ICCV 2021 as an <font color="#CC5500">Oral</font> paper (top 3% among all submissions). 
		</li>

		<p> 
		<li>
		[2021/06] Our ClipBERT paper wins the <font color="#FF0000">Best Student Paper Honorable Mention</font> Award at CVPR 2021. 
		</li>

		<p> 
		<li>
		[2021/06] 4 pieces of updates on our recent vision-and-language efforts: (i) Our <a style="color: #CC5500;" href="https://vqa2vln-tutorial.github.io/">CVPR 2021 tutorial</a> will happen on 6/20; (ii) Our <a style="color: #CC5500;" href="https://value-leaderboard.github.io/">VALUE benchmark and competition</a> has been launched; (iii) The arXiv version of our <a style="color: #CC5500;" href="https://arxiv.org/pdf/2106.00245.pdf">Adversarial VQA benchmark</a> has been released; (iv) We are the winner of <a style="color: #CC5500;" href="https://eval.ai/web/challenges/challenge-page/906/overview">TextCaps Challenge 2021</a>.
		</li>

		<p> 
		<li>
		[2021/05] Two papers accepted by ACL 2021: one long paper in the Main Conference track and the other in the Findings track. Topics include (i) EarlyBERT for efficient BERT training, and (ii) Cluster-Former as an efficient transformer variant for question answering.
		</li>

		<p> 
		<li>
		[2021/03] Two papers accepted by CVPR 2021, and one paper accepted to NAACL 2021. Topics include (i) ClipBERT for video-and-language learning (<font color="#CC5500">Oral with 3 strong accepts</font>), (ii) enhancing contrastive knowledge distillation with Wasserstein learning, and (iii) text generation from hyperbolic space.
		</li>

		<p> 
		<li>
		[2021/01] Our <a style="color: #CC5500;" href="https://arxiv.org/pdf/1910.03230.pdf">Meta Module Network</a> wins the <font color="#FF0000">Best Student Paper Honorable Mention</font> Award at WACV 2021.   
		</li>

		<p class="credits">&copy; September 2022 Zhe Gan<br />
		</ul>
	</div>
</body>
</html>
