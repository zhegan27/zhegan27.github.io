<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 2.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
	<meta name="description" content="Senior Researcher, Microsoft Cloud and AI" />
	<meta name="keywords" content="Zhe Gan, Deep Learning, Machine Learning, Natural Language Processing, Microsoft, Duke University, Peking University" />
	<meta name="Zhe Gan" content="Research" />
	<link rel="stylesheet" type="text/css" href="Zhe.css" title="Basic Profile" media="all" />
	<title>Zhe Gan</title>
</head>

<body>


	<div id="sidebar">
		<a href="index.html"><img src="images/Zhe_new.jpg"  height="180" alt="Sample logotype" /></a>
		<h1><a style="text-decoration: none" href="index.html">Zhe Gan</a></h1>
		<!-- <p class="slogan">everyone has a story to tell</p> -->
		
		<ul>
			<!--<li><a href="#">Page one</a><br />The front page...</li>-->
			<li><a href="Paper.html">Publications</a></li>
			<li><a href="https://scholar.google.com/citations?user=E64XWyMAAAAJ&hl=en">Google Scholar</a></li>
			<li><a href="https://github.com/zhegan27">GitHub</a></li>
			<li><a href="https://www.linkedin.com/pub/zhe-gan/78/29a/a22">LinkedIn</a></li>
			<li><a href="CV_Zhe.pdf">CV</a></li>
			<p>
			<font size="2">Senior Researcher<br>
			Microsoft Cloud and AI<br>
			Redmond, WA 98052<br>
			Email: zhe.gan@microsoft.com <br></font>
		</ul>
	</div>
	
	<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-35321140-3', â€˜zhegan.github.io');
  ga('send', 'pageview');

</script>
	
	<!--style="font-family: Calibri; color: blue; text-decoration: underline;"-->

	<div id="content">
		<!--<h2>What Starts Here Changes The World!</h2>-->
		<p>
		<p>
		<br>
		
		<p> I am a Senior Researcher at Microsoft Cloud and AI, primarily working on Vision-and-Language Representation Learning, and Generative Pre-training Models. I also have broad interests on various machine learning topics. I received my Ph.D. degree from <a style="color: #CC5500;" href="http://www.duke.edu/">Duke University</a> in Spring 2018. Before that, I received my Master's and B.Sc. from <a style="color: #CC5500;" href="http://www.pku.edu.cn/">Peking University</a> in 2013 and 2010, respectively. My Ph.D. advisor is <a style="color: #CC5500;" href="http://people.ee.duke.edu/~lcarin">Lawrence Carin</a>. I can be reached at zhe.gan@microsoft.com.


		<p> I will give a tutorial on Recent Advances in Vision-and-Language Research at <a style="color: #CC5500;" href="http://cvpr2020.thecvf.com/">CVPR 2020</a>. I will serve as an Area Chair for <a style="color: #CC5500;" href="https://nips.cc/Conferences/2020">NeurIPS 2020</a>. I will serve as an Interactive Arts Chair at <a style="color: #CC5500;" href="https://2020.acmmm.org/committee.html">ACM Multimedia 2020</a>. I received <a style="color: #CC5500;" href="https://aaai.org/Awards/conference.php">AAAI-20 Outstanding SPC Award</a>. I served as an Area Chair for <a style="color: #CC5500;" href="https://nips.cc/Conferences/2019">NeurIPS 2019</a>, and a Senior Program Committee (SPC) member for <a style="color: #CC5500;" href="https://aaai.org/Conferences/AAAI/aaai.php">AAAI 2020</a>.
		
		<p>
		<br/>
		
		<h3>Research Highlights:  </h3>
		<ul>

		<p> 
		<li>
		[2020/06] Please check out our new <a style="color: #CC5500;" href="https://arxiv.org/pdf/2006.06195.pdf">VILLA</a> paper on using adversarial training for vision-and-langauge representation learning in both pre-training and finetuning stages.   
		</li>

		<p> 
		<li>
		[2020/06] Two papers got accepted to ICML 2020. (i) CLUB: a novel upper bound of mutual information that is deeply connected with contrastive learning. (ii) GOT: a graph optimal transport framework for cross-domain alignment that can be used for V+L and NLP problems, such as VQA and NMT.   
		</li>

		<p> 
		<li>
		[2020/04] At this year's CVPR, we will give a tutorial on "Recent Advances in Vision-and-Language Research", covering the recent popular multi-modal pre-training methods and other topics. More details are provided in the website <a style="color: #CC5500;" href="https://rohit497.github.io/Recent-Advances-in-Vision-and-Language-Research/">here</a>.  
		</li>

		<p> 
		<li>
		[2020/04] Two CVPR and three ACL papers got accepted, respectively. CVPR papers cover topics including: (i) high-resolution image synthesis from salient object layout, and (ii) a new dataset for video-and-language understanding. ACL papers cover topics include: (i) text summarization based on discourse units, (ii) BERT for text generation, and (iii) text generation that models the distant future.    
		</li>

		<p> 
		<li>
		[2020/03] Will serve as an Area Chair for <a style="color: #CC5500;" href="https://nips.cc/Conferences/2020">NeurIPS 2020</a>.  
		</li>

		<p> 
		<li>
		[2020/01] I received AAAI-20 Outstanding SPC Award.
		</li>

		<p> 
		<li>
		[2019/11] Two papers accepted to AAAI 2020.
		</li>

		<p> 
		<li>
		[2019/09] Our new work <a style="color: #CC5500;" href="https://arxiv.org/pdf/1909.11740.pdf">UNITER</a> achieves SOTA on 6 Vision-and-Language tasks across 9 datasets (VQA, VCR, NLVR, Img-Txt Retrieval, Visual Entailment, Referring Expression).
		</li>

		<p> 
		<li>
		[2019/09] Our latest Adversarial Training model has beaten Facebook's RoBERTa on <a style="color: #CC5500;" href="https://gluebenchmark.com/leaderboard/">GLUE benchmark</a>. Paper is available <a style="color: #CC5500;" href="https://arxiv.org/pdf/1909.11764.pdf">here</a>.
		</li>

		<p> 
		<li>
		[2019/08] 4 papers got accepted to EMNLP. Topics include (i) BERT model compression, (ii) domain adaptation for MRC, (iii) domain adaptation for text style transfer, and (iv) image caption evaluation. 
		</li>

		<p> 
		<li>
		[2019/07] One paper got accepted to ICCV, using relation-aware graph attention for VQA. 
		</li>

		<p> 
		<li>
		[2019/06] Will serve as a Senior Program Committee (SPC) member for <a style="color: #CC5500;" href="https://aaai.org/Conferences/AAAI/aaai.php">AAAI 2020</a>.
		</li>

		<p> 
		<li>
		[2019/02] Will serve as an Area Chair for <a style="color: #CC5500;" href="https://nips.cc/Conferences/2019">NeurIPS 2019</a>.  
		</li>

		<p>
		<li>
		[2018/09] Two papers got accepted to NIPS 2018. The first one proposes Adversarial Information Maximization (AIM) for conversational response generation. The second one proposes feature mover GAN for neural text generation. The acceptance ratio this year is 1011/4856=20.82%.
		</li>
		
		<p> 
		<li>
		[2018/02/20] PhD thesis defended. 
		</li>

		<p> 
		<li>
		[2018/02] One paper accepted to CVPR 2018. GAN plus attention results in our AttnGAN, generates realistic images on birds and COCO datasets. See the news coverage here: <a style=" color: blue;" href="https://blogs.microsoft.com/ai/drawing-ai/">Drawing AI</a>, among other places. 
		</li>

		<p> 
		<li>
		[2017/09] 4 papers got accepted to NIPS 2017; three of them are on deep generative models, including VAE and GAN variants; the other one is on deconvolutional paragraph representation learning. The acceptance ratio of NIPS 2017 is 678/3240= 20.93%.
		</li>

		<p> 
		<li>
		[2017/05] I got two papers accepted to ICML this year. One proposes a new GAN model for text generation that handles the discrete nature of text inputs and uses adversarial feature matching, based on our NIPS workshop paper. The other proposes a new SG-MCMC algorithm. 
		</li>

		<p> 
		<li>
		[2017/03] Two papers on image captioning are accepted by CVPR 2017. One proposes semantic compositional network for visual captioning, and the other proposes StyleNet that can generate stylized captions. 
		</li>
		
		<p class="credits">&copy; June 2020 Zhe Gan<br />
		</ul>
	</div>
</body>
</html>
