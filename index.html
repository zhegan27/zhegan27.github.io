<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 2.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
	<meta name="description" content="Principal Researcher, Microsoft Azure AI" />
	<meta name="keywords" content="Zhe Gan, Deep Learning, Machine Learning, Natural Language Processing, Computer Vision, Microsoft, Duke University, Peking University" />
	<meta name="Zhe Gan" content="Research" />
	<link rel="stylesheet" type="text/css" href="Zhe.css" title="Basic Profile" media="all" />
	<title>Zhe Gan</title>
</head>

<body>


	<div id="sidebar">
		<a href="index.html"><img src="images/Zhe_new.jpg"  height="180" alt="Sample logotype" /></a>
		<h1><a style="text-decoration: none" href="index.html">Zhe Gan</a></h1>
		<!-- <p class="slogan">everyone has a story to tell</p> -->
		
		<ul>
			<!--<li><a href="#">Page one</a><br />The front page...</li>-->
			<li><a href="Paper.html">Publications</a></li>
			<li><a href="https://scholar.google.com/citations?user=E64XWyMAAAAJ&hl=en">Google Scholar</a></li>
			<li><a href="https://github.com/zhegan27">GitHub</a></li>
			<li><a href="https://www.linkedin.com/pub/zhe-gan/78/29a/a22">LinkedIn</a></li>
			<li><a href="CV_Zhe.pdf">CV</a></li>
			<p>
			<font size="2">Principal Researcher<br>
			Microsoft Azure AI<br>
			Redmond, WA 98052<br>
			Email: zhe.gan@microsoft.com <br></font>
		</ul>
	</div>
	
	<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-35321140-3', â€˜zhegan.github.io');
  ga('send', 'pageview');

</script>
	
	<!--style="font-family: Calibri; color: blue; text-decoration: underline;"-->

	<div id="content">
		<!--<h2>What Starts Here Changes The World!</h2>-->
		<p>
		<p>
		<br>
		
		<p> I am a Principal Researcher at Microsoft Azure AI, primarily working on Vision-and-Language Multimodal Intelligence, a research area that sits at the nexus of computer vision and natural language processing, such as Vision-and-Language Pre-training (VLP), Visual Question Answering (VQA), image captioning, and video-text modeling. I also have broad interests on other machine learning topics, such as sparse neural networks, adversarial training, and self-supervised visual representation learning. I received my Ph.D. degree from <a style="color: #CC5500;" href="http://www.duke.edu/">Duke University</a> in Spring 2018. Before that, I received my Master's and B.Sc. from <a style="color: #CC5500;" href="http://www.pku.edu.cn/">Peking University</a> in 2013 and 2010, respectively. My Ph.D. advisor is <a style="color: #CC5500;" href="http://people.ee.duke.edu/~lcarin">Lawrence Carin</a>. I can be reached at zhe.gan@microsoft.com.


		<p> I am serving (or, has served) as an Area Chair for <a style="color: #CC5500;" href="https://nips.cc/Conferences/2021">NeurIPS 2021/2020/2019</a>, <a style="color: #CC5500;" href="https://icml.cc/Conferences/2021">ICML 2021</a>, <a style="color: #CC5500;" href="https://iclr.cc/">ICLR 2021</a>, <a style="color: #CC5500;" href="https://2022.aclweb.org/">ACL 2022/2021</a>, <a style="color: #CC5500;" href="https://aaai.org/Conferences/AAAI/aaai.php">AAAI 2022</a>, and a Senior Program Committee (SPC) member for <a style="color: #CC5500;" href="https://aaai.org/Conferences/AAAI/aaai.php">AAAI 2021/2020</a>, and received <a style="color: #CC5500;" href="https://aaai.org/Awards/conference.php">AAAI-20 Outstanding SPC Award</a>. 
		
		<p>
		<br/>
		
		<h3>Research Highlights:  </h3>
		<ul>

		<p> 
		<li>
		[2021/10] 3 papers accepted to the main conference track of NeurIPS 2021, including Sparse ViT, Elastic LTH, and GAN lottery tickets; and 2 papers accepted to the datasets and benchmarks track of NeurIPS 2021, including AdvGLUE and VALUE. 
		</li>

		<p> 
		<li>
		[2021/10] During the summer, we have hosted a special Vision-Language Talk Series. With 11 invited speakers from both academia and industry, we have covered diverse topics ranging from image captioning, VQA, multimodal pre-training (ALIGN, MDETR), grounded visual generation, zero-shot object detection (ViLD), video-language understanding (MERLOT), to self-supervised learning (MoCo-v3). Want to know more?
		Please check the <a style="color: #CC5500;" href="https://www.youtube.com/playlist?list=PLD7HFcN7LXReRU4tDhN4VPTl4BpyTKAco">YouTube playlist</a> and <a style="color: #CC5500;" href="https://www.microsoft.com/en-us/research/videos/vision-language-summer-talk-series/">MSR video series</a>.
		</li>

		<p> 
		<li>
		[2021/09] We all know GPT-3 is a strong few-shot learner for NLP problems, but can it also benefit multimodal tasks? In <a style="color: #CC5500;" href="https://arxiv.org/pdf/2109.05014.pdf">this new work</a>, we provide an empirical study of GPT-3 for knowledge-based VQA, and show that prompting GPT-3 via the use of image captions with only 16 examples surpasses supervised sota by an absolute +8.6 points on the OK-VQA dataset (from 39.4 to 48.0). 
		</li>

		<p> 
		<li>
		[2021/07] Our Adversarial VQA work is accepted by ICCV 2021 as an <font color="#CC5500">Oral</font> paper (top 3% among all submissions). 
		</li>

		<p> 
		<li>
		[2021/06] Our ClipBERT paper wins the <font color="#FF0000">Best Student Paper Honorable Mention</font> Award at CVPR 2021. 
		</li>

		<p> 
		<li>
		[2021/06] 4 pieces of updates on our recent vision-and-language efforts: (i) Our <a style="color: #CC5500;" href="https://vqa2vln-tutorial.github.io/">CVPR 2021 tutorial</a> will happen on 6/20; (ii) Our <a style="color: #CC5500;" href="https://value-leaderboard.github.io/">VALUE benchmark and competition</a> has been launched; (iii) The arXiv version of our <a style="color: #CC5500;" href="https://arxiv.org/pdf/2106.00245.pdf">Adversarial VQA benchmark</a> has been released; (iv) We are the winner of <a style="color: #CC5500;" href="https://eval.ai/web/challenges/challenge-page/906/overview">TextCaps Challenge 2021</a>.
		</li>

		<p> 
		<li>
		[2021/05] Two papers accepted by ACL 2021: one long paper in the Main Conference track and the other in the Findings track. Topics include (i) EarlyBERT for efficient BERT training, and (ii) Cluster-Former as an efficient transformer variant for question answering.
		</li>

		<p> 
		<li>
		[2021/03] Two papers accepted by CVPR 2021, and one paper accepted to NAACL 2021. Topics include (i) ClipBERT for video-and-language learning (<font color="#CC5500">Oral with 3 strong accepts</font>), (ii) enhancing contrastive knowledge distillation with Wasserstein learning, and (iii) text generation from hyperbolic space.
		</li>

		<p> 
		<li>
		[2021/02] We will host a tutorial at CVPR 2021: <a style="color: #CC5500;" href="https://vqa2vln-tutorial.github.io/">From VQA to VLN: Recent Advances in Vision-and-Language Research</a>. This year, we decide to extend our half-day event to a full one with enriched contents. Organizers include Peter Anderson, Yoav Artzi, Xiaodong He, Linjie Li, Jingjing Liu, Xin (Eric) Wang, Qi Wu, Luowei Zhou, and me.   
		</li>

		<p> 
		<li>
		[2021/01] Two papers accepted by ICLR 2021. Topics include using information-theoretic tools for (i) improved robustness of language models (BERT and RoBERTa) and (ii) zero-shot voice style transfer.
		</li>

		<p> 
		<li>
		[2021/01] Our <a style="color: #CC5500;" href="https://arxiv.org/pdf/1910.03230.pdf">Meta Module Network</a> wins the <font color="#FF0000">Best Student Paper Honorable Mention</font> Award at WACV 2021.   
		</li>

		<p> 
		<li>
		[2020/09] Our <a style="color: #CC5500;" href="https://arxiv.org/pdf/2006.06195.pdf">VILLA</a> paper got accepted to NeurIPS 2020 as a Spotlight paper with review scores 8887. It is the first known effort that studies large-scale adversarial training for vision-and-langauge representation learning in both pre-training and finetuning stages.   
		</li>

		<p class="credits">&copy; November 2021 Zhe Gan<br />
		</ul>
	</div>
</body>
</html>
