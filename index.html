<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 2.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
	<meta name="description" content="Research Scientist, Apple AI/ML" />
	<meta name="keywords" content="Zhe Gan, Deep Learning, Machine Learning, Natural Language Processing, Computer Vision, Apple, Microsoft, Duke University, Peking University" />
	<meta name="Zhe Gan" content="Research" />
	<link rel="stylesheet" type="text/css" href="Zhe.css" title="Basic Profile" media="all" />
	<title>Zhe Gan</title>
</head>

<body>


	<div id="sidebar">
		<a href="index.html"><img src="images/Zhe_new2.jpeg"  height="180" alt="Sample logotype" /></a>
		<h1><a style="text-decoration: none" href="index.html">Zhe Gan</a></h1>
		<!-- <p class="slogan">everyone has a story to tell</p> -->
		
		<ul>
			<!--<li><a href="#">Page one</a><br />The front page...</li>-->
			<li><a href="Paper.html">Publications</a></li>
			<li><a href="https://scholar.google.com/citations?user=E64XWyMAAAAJ&hl=en">Google Scholar</a></li>
			<li><a href="https://github.com/zhegan27">GitHub</a></li>
			<li><a href="https://www.linkedin.com/pub/zhe-gan/78/29a/a22">LinkedIn</a></li>
			<li><a href="CV_Zhe.pdf">CV</a></li>
			<p>
			<font size="2">Research Scientist and Manager<br>
			Apple AI/ML<br>
			Seattle, WA 98109<br>
			Email: zhe.gan@apple.com <br></font>
		</ul>
	</div>
	
	<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-35321140-3', â€˜zhegan.github.io');
  ga('send', 'pageview');

</script>
	
	<!--style="font-family: Calibri; color: blue; text-decoration: underline;"-->

	<div id="content">
		<!--<h2>What Starts Here Changes The World!</h2>-->
		<p>
		<p>
		<br>
		
		<p> I am a Research Scientist and Manager at Apple AI/ML, primarily working on building large-scale vision and multimodal foundation models. Before joining Apple, I was a Principal Researcher at Microsoft Azure AI, working on <a style="color: #CC5500;" href="https://www.microsoft.com/en-us/research/project/project-florence-vl/">Project Florence-VL</a>. I received my Ph.D. degree from <a style="color: #CC5500;" href="http://www.duke.edu/">Duke University</a> in Spring 2018, and my Master's and B.Sc. degree from <a style="color: #CC5500;" href="http://www.pku.edu.cn/">Peking University</a> in 2013 and 2010, respectively. My Ph.D. advisor is <a style="color: #CC5500;" href="http://people.ee.duke.edu/~lcarin">Lawrence Carin</a>. I can be reached at pkuganzhe@gmail.com and zhe.gan@apple.com.


		<p> I am serving (or, has served) as a Senior Area Chair (SAC) for <a style="color: #CC5500;" href="https://2025.aclweb.org/">ACL 2025</a>, <a style="color: #CC5500;" href="https://2024.emnlp.org/">EMNLP 2024</a>, an Area Chair for <a style="color: #CC5500;" href="https://nips.cc/Conferences/2025">NeurIPS 2019-2025</a>, <a style="color: #CC5500;" href="https://icml.cc/Conferences/2025">ICML 2021-2025</a>, <a style="color: #CC5500;" href="https://iclr.cc/">ICLR 2021-2025</a>, <a style="color: #CC5500;" href="https://cvpr2025.thecvf.com/">CVPR 2023-2025</a>, <a style="color: #CC5500;" href="https://https://iccv2025.thecvf.com/">ICCV 2025</a>, <a style="color: #CC5500;" href="https://eccv2022.ecva.net/">ECCV 2022</a>, <a style="color: #CC5500;" href="https://wacv2024.thecvf.com/">WACV 2024</a>, <a style="color: #CC5500;" href="https://2024.aclweb.org/">ACL 2021-2024</a>, <a style="color: #CC5500;" href="https://2024.naacl.org/">NAACL 2024/2022</a>, <a style="color: #CC5500;" href="https://2023.emnlp.org/">EMNLP 2023/2022</a>, <a style="color: #CC5500;" href="https://colmweb.org/">COLM 2025/2024</a>,  <a style="color: #CC5500;" href="https://aaai.org/Conferences/AAAI/aaai.php">AAAI 2023/2022</a>, and a Senior Program Committee (SPC) member for <a style="color: #CC5500;" href="https://aaai.org/Conferences/AAAI/aaai.php">AAAI 2021/2020</a>, and received <a style="color: #CC5500;" href="https://aaai.org/Awards/conference.php">AAAI-20 Outstanding SPC Award</a>. Together with my co-authors, I have also been honored with the Best Student Paper Honorable Mention Awards at CVPR 2021 and WACV 2021, respectively.
		
		<p>
		<br/>
		
		<h3>Research Highlights:  </h3>
		<ul>

		<p> 
		<li>
		[2025/3] Please checkout our recent work <a style="color: #CC5500;" href="https://arxiv.org/pdf/2503.12652">UniVG</a>, a generalist diffusion model for unified image generation and editing.
		</li>

		<p> 
		<li>
		[2025/2] 2 papers accepted to CVPR 2025: (1) <a style="color: #CC5500;" href="https://arxiv.org/pdf/2411.14402">AIMv2</a>, an exploration of multimodal autoregressive pre-training of large vision encoders; and (2) <a style="color: #CC5500;" href="https://www.arxiv.org/pdf/2412.08442">GEA</a>, an empirical analysis of both data and method choices in agents across robotics, planning, UI interactions, and video games.
		</li>

		<p> 
		<li>
		[2025/1] 5 papers accepted to ICLR 2025: (1) <a style="color: #CC5500;" href="https://arxiv.org/pdf/2409.20566">MM1.5</a>, a significant upgrade of MM1; (2) <a style="color: #CC5500;" href="https://arxiv.org/pdf/2410.18967">Ferret-UI 2</a>, a upgrade of Ferret-UI for universal UI understanding; (3) <a style="color: #CC5500;" href="https://arxiv.org/pdf/2410.07177">MM-Ego</a>, an exploration of egocentric multimodal LLMs; (4) <a style="color: #CC5500;" href="https://arxiv.org/pdf/2410.02740">VeCapV2</a>, our upgrade of VeCap, and a comprehensive study of synethic image captions across CLIP, multimodal LLM, and diffusion models; and (5) <a style="color: #CC5500;" href="https://arxiv.org/pdf/2407.01509">MIA-Bench</a>, a new benchmark that aims at better instruction following evaluation of multimodal LLMs.
		</li>


		<p> 
		<li>
		[2024/10] Please checkout our new papers: (1) Chain-of-thought reasoning for multimodal LLMs (<a style="color: #CC5500;" href="https://arxiv.org/pdf/2410.16198">paper</a>); (2) <a style="color: #CC5500;" href="https://arxiv.org/pdf/2410.02746">CLOC</a>, our next-generation image encoder for multimodal LLM; (3) <a style="color: #CC5500;" href="https://arxiv.org/pdf/2407.15841">SlowFast-LLaVA</a>, an exploration of the slow-fast idea for video LLMs; and (4) a comprehensive study of alignment in multimodal LLMs (<a style="color: #CC5500;" href="https://arxiv.org/pdf/2407.02477">paper</a>). 
		</li>

		<p> 
		<li>
		[2024/7] 4 papers accepted to ECCV 2024: <a style="color: #CC5500;" href="https://arxiv.org/abs/2403.09611">MM1</a>, <a style="color: #CC5500;" href="https://arxiv.org/pdf/2404.05719">Ferret-UI</a>, <a style="color: #CC5500;" href="https://arxiv.org/pdf/2310.07699">VeCLIP</a>, and <a style="color: #CC5500;" href="https://arxiv.org/pdf/2212.00280">GRiT</a>. <a style="color: #CC5500;" href="https://arxiv.org/pdf/2404.07973.pdf">Ferret-v2</a> got accepted to COLM 2024. Also, please checkout my talk on MM1 at CVPR 2024 <a style="color: #CC5500;" href="https://vlp-tutorial.github.io/">tutorial</a> and <a style="color: #CC5500;" href="https://computer-vision-in-the-wild.github.io/cvpr-2024/">workshop</a> (<a style="color: #CC5500;" href="https://datarelease.blob.core.windows.net/tutorial/vision_foundation_models_2024/Zhe_pretraining.pdf">slides</a>).
		</li>

		<p> 
		<li>
		[2024/3] Please checkout our new paper <a style="color: #CC5500;" href="https://arxiv.org/abs/2403.09611">MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training</a>.
		</li>

		<p> 
		<li>
		[2024/2] 4 papers accepted to ICLR 2024: (1) Ferret: a new multimodal LLM that can refer and ground anything anywhere at any granularity (<a style="color: #CC5500;" href="https://arxiv.org/pdf/2310.07704.pdf">here</a>), (2) MGIE: multimodal LLM for guiding instruction-based image editing (<a style="color: #CC5500;" href="https://arxiv.org/pdf/2309.17102.pdf">here</a>), (3) compressing LLMs: the truth is rarely pure and never simple (<a style="color: #CC5500;" href="https://arxiv.org/pdf/2310.01382.pdf">here</a>), and (4) MOFI: learning image representations from noisy entity annotated images (<a style="color: #CC5500;" href="https://arxiv.org/pdf/2306.07952.pdf">here</a>).
		</li>

		<p> 
		<li>
		[2023/9] Please checkout our survey paper/book on  
		<a style="color: #CC5500;" href="https://arxiv.org/pdf/2309.10020.pdf">Multimodal Foundation Models: From Specialists to General-Purpose Assistants</a>. 
		</li>

		<p> 
		<li>
		[2023/6] We held a tutorial on Recent Advances in Vision Foundation Models at CVPR 2023. All the slides can now be downloaded from the 
		<a style="color: #CC5500;" href="https://vlp-tutorial.github.io/">tutorial webpage</a>. 
		</li>

		<p> 
		<li>
		[2023/6] <a style="color: #CC5500;" href="https://arxiv.org/abs/2306.07952">MOFI</a> is our new vision foundation model that is designed to learn image representations from noisy entity annotated images. To achieve this, we have created Image-to-Entities (I2E), a new large-scale dataset with 1 billion images and 2 million distinct entities, covering rich visual concepts in the wild.
		</li>

		<p> 
		<li>
		[2023/2] 5 papers accepted to CVPR 2023: (1) X-decoder: generalist modeling for (open-vocab) segmentation and vision-language tasks; (2) ReCo: region-controlled text-to-image generation; (3) x-CLIP: enhancing CLIP with non-contrastive learning; (4) LAVENDER and VIOLET-v2: two empirical studies on video-language pre-training.
		</li>

		<p> 
		<li>
		[2023/1] Our recent work on <a style="color: #CC5500;" href="https://arxiv.org/pdf/2210.09150.pdf">Prompting GPT-3 To Be Reliable</a> got accepted to ICLR 2023.
		</li>

		<p> 
		<li>
		[2023/1] Gave an invited talk on multimodal foundation models at <a style="color: #CC5500;" href="https://asu-apg.github.io/serum/">WACV 2023</a> [<a style="color: #CC5500;" href="https://www.dropbox.com/s/8utdqq2jhqdz5kt/WACVTalk.2023.1.7.pdf?dl=0">slides</a>].
		</li>

		<p> 
		<li>
		[2022/12] Please check out our new survey paper/book on <a style="color: #CC5500;" href="https://arxiv.org/pdf/2210.09263.pdf">Vision-Language Pre-training: Basics, Recent Advances, and Future Trends</a>, published at <a style="color: #CC5500;" href="https://www.nowpublishers.com/article/Details/CGV-105">Foundations and Trends in Computer Graphics and Vision</a>.
		</li>


		<p class="credits">&copy; March 2025 Zhe Gan<br />
		</ul>
	</div>
</body>
</html>
