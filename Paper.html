<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 2.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
	<meta name="description" content="Research Scientist, Apple AI/ML" />
	<meta name="keywords" content="Zhe Gan, Deep Learning, Machine Learning, Natural Language Processing, Computer Vision, Apple, Microsoft, Duke University, Peking University" />
	<meta name="Zhe Gan" content="Research" />
	<link rel="stylesheet" type="text/css" href="Zhe.css" title="Basic Profile" media="all" />
	<title>Zhe Gan</title>
</head>

<body>




	<div id="sidebar">
		<a href="index.html"><img src="images/Zhe_new2.jpeg"  height="180" alt="Sample logotype" /></a>
		<h1><a style="text-decoration: none" href="index.html">Zhe Gan</a></h1>
		<!-- <p class="slogan">everyone has a story to tell</p> -->
		
		<ul>
			<!--<li><a href="#">Page one</a><br />The front page...</li>-->
			<li><a href="Paper.html">Publications</a></li>
			<li><a href="https://scholar.google.com/citations?user=E64XWyMAAAAJ&hl=en">Google Scholar</a></li>
			<li><a href="https://github.com/zhegan27">GitHub</a></li>
			<li><a href="https://www.linkedin.com/pub/zhe-gan/78/29a/a22">LinkedIn</a></li>
			<li><a href="CV_Zhe.pdf">CV</a></li>
			<p>
			<font size="2">Staff Research Scientist<br>
			Apple AI/ML<br>
			Seattle, WA 98109<br>
			Email: zhe.gan@apple.com <br></font>

		</ul>
	</div>
	
	<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-35321140-3', ‘zhegan27.github.io’);
  ga('send', 'pageview’);

</script>
	
	<!--style="font-family: Calibri; color: blue; text-decoration: underline;"-->

	<div id="content">
		<!--<h2>What Starts Here Changes The World!</h2>-->
		<p>
		<p>
		<br>
		
		<h2>Publications </h2>

<ul class="articlesubtitle" style="margin-bottom: -0.01in;">
<strong>arXiv preprints</strong>
</ul>

<ul class="articlesubtitle" style="margin-left: 0.2in;">
<li> Zhengfeng Lai*, Haotian Zhang*, Wentao Wu, Haoping Bai, Aleksei Timofeev, Xianzhi Du, <strong>Zhe Gan</strong>, Jiulong Shan, Chen-Nee Chuah, Yinfei Yang and Meng Cao “From Scarcity to Efficiency: Improving CLIP Training via Visual-enriched Captions”, 2023. <a style=" color: blue;" href="https://arxiv.org/pdf/2310.07699.pdf">PDF</a> 
</li>

<li> Jaemin Cho, Linjie Li, Zhengyuan Yang, <strong>Zhe Gan</strong>, Lijuan Wang and Mohit Bansal “Diagnostic Benchmark and Iterative Inpainting for Layout-Guided Image Generation”, 2023. <a style=" color: blue;" href="https://arxiv.org/pdf/2304.06671.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://layoutbench.github.io/">Project page</a> 
</li>

<li> Jialian Wu, Jianfeng Wang, Zhengyuan Yang, <strong>Zhe Gan</strong>, Zicheng Liu, Junsong Yuan and Lijuan Wang “GRiT: A Generative Region-to-text Transformer for Object Understanding”, 2023. <a style=" color: blue;" href="https://arxiv.org/pdf/2212.00280.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/JialianW/GRiT">Code</a> 
</li>

<li> Bingbing Wen, Zhengyuan Yang, Jianfeng Wang, <strong>Zhe Gan</strong>, Bill Howe and Lijuan Wang “InfoVisDial: An Informative Visual Dialogue Dataset by Bridging Large Multimodal and Language Models”, 2023. <a style=" color: blue;" href="https://arxiv.org/pdf/2312.13503.pdf">PDF</a> 
</li>

</li>

</li>

</ul>

<ul class="articlesubtitle" style="margin-top: -0.25in; margin-bottom: -0.01in;">
<strong>2024</strong>
</ul>

<ul class="articlesubtitle" style="margin-left: 0.2in;">
<li> Haoxuan You*, Haotian Zhang*, <strong>Zhe Gan</strong>, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang and Yinfei Yang “Ferret: Refer and Ground Anything Anywhere at Any Granularity”, Int. Conf. Learning Representations (<strong>ICLR</strong>), 2024. <a style=" color: blue;" href="https://arxiv.org/pdf/2310.07704.pdf">PDF</a> (<font color="#FF0000"><strong>Spotlight, Top 5% among all submissions</strong></font>)
</li>

<li> Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang and <strong>Zhe Gan</strong> “Guiding Instruction-based Image Editing via Multimodal Large Language Models”, Int. Conf. Learning Representations (<strong>ICLR</strong>), 2024. <a style=" color: blue;" href="https://arxiv.org/pdf/2309.17102.pdf">PDF</a> (<font color="#FF0000"><strong>Spotlight, Top 5% among all submissions</strong></font>)
</li>

<li> Ajay Jaiswal, <strong>Zhe Gan</strong>, Xianzhi Du, Bowen Zhang, Zhangyang Wang and Yinfei Yang “Compressing LLMs: The Truth is Rarely Pure and Never Simple”, Int. Conf. Learning Representations (<strong>ICLR</strong>), 2024. <a style=" color: blue;" href="https://arxiv.org/pdf/2310.01382.pdf">PDF</a> 
</li>

<li> Wentao Wu*, Aleksei Timofeev*, Chen Chen, Bowen Zhang, Kun Duan, Shuangning Liu, Yantao Zheng, Jonathon Shlens, Xianzhi Du, <strong>Zhe Gan</strong> and Yinfei Yang “MOFI: Learning Image Representations from Noisy Entity Annotated Images”, Int. Conf. Learning Representations (<strong>ICLR</strong>), 2024. <a style=" color: blue;" href="https://arxiv.org/pdf/2306.07952.pdf">PDF</a> 
</li>

</li>

</li>

</ul>

<ul class="articlesubtitle" style="margin-top: -0.25in; margin-bottom: -0.01in;">
<strong>2023</strong>
</ul>

<ul class="articlesubtitle" style="margin-left: 0.2in;">
<li> Chunyuan Li*, <strong>Zhe Gan</strong>*, Zhengyuan Yang*, Jianwei Yang*, Linjie Li*, Lijuan Wang and Jianfeng Gao “Multimodal Foundation Models: From Specialists to General-Purpose Assistants”, Foundations and Trends in Computer Graphics and Vision, 2023. <a style=" color: blue;" href="https://arxiv.org/pdf/2309.10020.pdf">PDF</a> (<font color="#CC5500"><strong>A survey book on multimodal foundation models</strong></font>)
</li>

<li> Yuhui Zhang, Brandon McKinzie, <strong>Zhe Gan</strong>, Vaishaal Shankar and Alexander Toshev “Pre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation”, Neural Information Processing Systems (<strong>NeurIPS</strong>), Workshop on I Can't Believe It's Not Better, 2023. <a style=" color: blue;" href="https://arxiv.org/pdf/2311.16201.pdf">PDF</a> 
</li>

<li> Yi-Lin Sung, Linjie Li, Kevin Lin, <strong>Zhe Gan</strong>, Mohit Bansal and Lijuan Wang “An Empirical Study of Multimodal Model Merging”, Conf. on Empirical Methods in Natural Language Processing (<strong>Findings of EMNLP</strong>), 2023. <a style=" color: blue;" href="https://arxiv.org/pdf/2304.14933.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/ylsung/vl-merging">Code</a> 
</li>

<li> Xueyan Zou*, Zi-Yi Dou*, Jianwei Yang*, <strong>Zhe Gan</strong>, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu Yuan, Nanyun Peng, Lijuan Wang, Yong Jae Lee and Jianfeng Gao “Generalized Decoding for Pixel, Image, and Language”, Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023. <a style=" color: blue;" href="https://arxiv.org/pdf/2212.11270.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/microsoft/X-Decoder">Code</a> /
<a style=" color: #CC5500;" href="https://x-decoder-vl.github.io">Project page</a>
</li>

<li> Zhengyuan Yang, Jianfeng Wang, <strong>Zhe Gan</strong>, Linjie Li, Kevin Lin, Chenfei Wu, Nan Duan, Zicheng Liu, Ce Liu, Michael Zeng and Lijuan Wang “ReCo: Region-Controlled Text-to-Image Generation”, Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023. <a style=" color: blue;" href="https://arxiv.org/pdf/2211.15518.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/microsoft/ReCo">Code</a> 
</li>

<li> Jinghao Zhou, Li Dong, <strong>Zhe Gan</strong>, Lijuan Wang and Furu Wei “Non-Contrastive Learning Meets Language-Image Pre-Training”, Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023. <a style=" color: blue;" href="https://arxiv.org/pdf/2210.09304.pdf">PDF</a>
</li>

<li> Linjie Li, <strong>Zhe Gan</strong>, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Ce Liu and Lijuan Wang “LAVENDER: Unifying Video-Language Understanding as Masked Language Modeling”, Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023. <a style=" color: blue;" href="https://arxiv.org/pdf/2206.07160.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/microsoft/LAVENDER">Code</a>
</li>

<li> Tsu-Jui Fu*, Linjie Li*, <strong>Zhe Gan</strong>, Kevin Lin, William Yang Wang, Lijuan Wang and Zicheng Liu “An Empirical Study of End-to-End Video-Language Transformers with Masked Visual Modeling”, Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023. <a style=" color: blue;" href="https://arxiv.org/pdf/2209.01540.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/tsujuifu/pytorch_empirical-mvm">Code</a>
</li>

<li> Chenglei Si, <strong>Zhe Gan</strong>, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber and Lijuan Wang “Prompting GPT-3 To Be Reliable”, Int. Conf. Learning Representations (<strong>ICLR</strong>), 2023. <a style=" color: blue;" href="https://arxiv.org/pdf/2210.09150.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/NoviScl/GPT3-Reliability">Code</a>
</li>

<li> Zixin Zhu*, Yixuan Wei*, Jianfeng Wang, <strong>Zhe Gan</strong>, Zheng Zhang, Le Wang, Gang Hua, Lijuan Wang, Zicheng Liu and Han Hu “Exploring Discrete Diffusion Models for Image Captioning”, arXiv preprint, 2023. <a style=" color: blue;" href="https://arxiv.org/pdf/2211.11694.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/buxiangzhiren/DDCap">Code</a> 
</li>

</ul>

<ul class="articlesubtitle" style="margin-top: -0.25in; margin-bottom: -0.01in;">
<strong>2022</strong>
</ul>

<ul class="articlesubtitle" style="margin-left: 0.2in;">

<li> <strong>Zhe Gan</strong>, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu and Jianfeng Gao “Vision-Language Pre-training: Basics, Recent Advances, and Future Trends”, Foundations and Trends in Computer Graphics and Vision, 2022. <a style=" color: blue;" href="https://arxiv.org/pdf/2210.09263.pdf">PDF</a> /
<a style=" color: blue;" href="https://www.nowpublishers.com/article/Details/CGV-105">Link</a>
(<font color="#CC5500"><strong>A survey book on vision-language pre-training</strong></font>)
</li>

<li> Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, <strong>Zhe Gan</strong>, Zicheng Liu, Ce Liu and Lijuan Wang “GIT: A Generative Image-to-text Transformer for Vision and Language”, Transactions on Machine Learning Research (<strong>TMLR</strong>), 2022.  <a style=" color: blue;" href="https://arxiv.org/pdf/2205.14100.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/microsoft/GenerativeImage2Text">Code</a> 
(<font color="#CC5500"><strong>Our new multimodal foundation model that achieves 12 new SOTA on a diverse set of image/video captioning and QA tasks</strong></font>)
</li>

<li> Chenfei Wu*, Jian Liang*, Xiaowei Hu, <strong>Zhe Gan</strong>, Jianfeng Wang, Lijuan Wang, Zicheng Liu, Yuejian Fang and Nan Duan “NUWA-Infinity: Autoregressive over Autoregressive Generation for Infinite Visual Synthesis”, Neural Information Processing Systems (<strong>NeurIPS</strong>), 2022. <a style=" color: blue;" href="https://arxiv.org/pdf/2207.09814.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://nuwa-infinity.microsoft.com/">Webpage 1</a> /
<a style=" color: #CC5500;" href="https://www.microsoft.com/en-us/research/project/nuwa-infinity/">Webpage 2</a> /
<a style=" color: #CC5500;" href="https://github.com/microsoft/NUWA">GitHub</a> /
<a style=" color: #CC5500;" href="https://twitter.com/_akhaliq/status/1549954767585173505?s=21&t=dBZGCvHFEf_KT5iNZs3MYw">Twitter</a> /
<a style=" color: #CC5500;" href="https://www.youtube.com/watch?v=_KvGSf1y0MU">YouTube</a>
</li>

<li> Zi-Yi Dou*, Aishwarya Kamath*, <strong>Zhe Gan</strong>*, Pengchuan Zhang, Jianfeng Wang, Linjie Li, Zicheng Liu, Ce Liu, Yann LeCun, Nanyun Peng, Jianfeng Gao and Lijuan Wang “Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone”, Neural Information Processing Systems (<strong>NeurIPS</strong>), 2022. <a style=" color: blue;" href="https://arxiv.org/pdf/2206.07643.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/microsoft/FIBER">Code</a> / 
<a style=" color: #CC5500;" href="https://ashkamath.github.io/FIBER_page/">Webpage</a> 
</li>

<li> Sheng Shen*, Chunyuan Li*, Xiaowei Hu*, Yujia Xie, Jianwei Yang, Pengchuan Zhang, <strong>Zhe Gan</strong>, Lijuan Wang, Lu Yuan, Ce Liu, Kurt Keutzer, Trevor Darrell, Anna Rohrbach and Jianfeng Gao “K-LITE: Learning Transferable Visual Models with External Knowledge”, Neural Information Processing Systems (<strong>NeurIPS</strong>), 2022. <a style=" color: blue;" href="https://arxiv.org/pdf/2204.09222.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/microsoft/klite">Code</a>
(<font color="#CC5500"><strong>Oral</strong></font>)
</li>

<li> Zhengyuan Yang, <strong>Zhe Gan</strong>, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao Lu and Lijuan Wang “UniTAB: Unifying Text and Box Outputs for Grounded Vision-Language Modeling”, European Conf. on Computer Vision (<strong>ECCV</strong>), 2022. <a style=" color: blue;" href="https://arxiv.org/pdf/2111.12085.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/microsoft/UniTAB">Code</a> 
(<font color="#CC5500"><strong>Oral, Top 2.7% among all submissions</strong></font>)
</li>

<li> Tianlong Chen, Yu Cheng, <strong>Zhe Gan</strong>, Jianfeng Wang, Lijuan Wang, Jingjing Liu and Zhangyang Wang  “Adversarial Feature Augmentation and Normalization for Visual Recognition”, Transactions on Machine Learning Research (<strong>TMLR</strong>), 2022. <a style=" color: blue;" href="https://arxiv.org/pdf/2103.12171.pdf">PDF</a> /
<a style=" color: blue;" href="https://openreview.net/pdf?id=j6rILItz4yr">Old</a> /
<a style=" color: #CC5500;" href="https://github.com/VITA-Group/CV_A-FAN">Code</a> 
</li>

<li> Zi-Yi Dou, Yichong Xu, <strong>Zhe Gan</strong>, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Pengchuan Zhang, Lu Yuan, Nanyun (Violet) Peng, Zicheng Liu and Michael Zeng “An Empirical Study of Training End-to-End Vision-and-Language Transformers”, Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2022. 
<a style=" color: blue;" href="https://arxiv.org/pdf/2111.02387.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/zdou0830/METER">Code</a> 
</li>

<li> Xiaowei Hu, <strong>Zhe Gan</strong>, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu and Lijuan Wang “Scaling Up Vision-Language Pre-training for Image Captioning”, Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2022. <a style=" color: blue;" href="https://arxiv.org/pdf/2111.12233.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/xiaoweihu/ALT200M">Code</a> 
</li>

<li> Kevin Lin*, Linjie Li*, Chung-Ching Lin*, Faisal Ahmed, <strong>Zhe Gan</strong>, Zicheng Liu, Yumao Lu and Lijuan Wang “SwinBERT: End-to-End Transformers with Sparse Attention for Video Captioning”, Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2022. <a style=" color: blue;" href="https://arxiv.org/pdf/2111.13196.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/microsoft/SwinBERT">Code</a> 
</li>

<li> Zhiyuan Fang, Jianfeng Wang, Xiaowei Hu, Lin Liang, <strong>Zhe Gan</strong>, Lijuan Wang, Yezhou Yang and Zicheng Liu “Injecting Semantic Concepts into End-to-End Image Captioning”, Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2022. <a style=" color: blue;" href="https://arxiv.org/pdf/2112.05230.pdf">PDF</a>
</li>

<li> Zhengyuan Yang, <strong>Zhe Gan</strong>, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu and Lijuan Wang “An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA”, Proc. American Association of Artificial Intelligence (<strong>AAAI</strong>), 2022. 
<a style=" color: blue;" href="https://arxiv.org/pdf/2109.05014.pdf">PDF</a> /
<a style=" color: blue;" href="Papers/GPT3_for_VQA_slides.pdf">Slides</a>
(<font color="#CC5500"><strong>Oral, Leaderboard #1 on OK-VQA as of Nov. 4, 2021</strong></font>)
</li>

<li> <strong>Zhe Gan</strong>, Yen-Chun Chen, Linjie Li, Tianlong Chen, Yu Cheng, Shuohang Wang, Jingjing Liu, Lijuan Wang and Zicheng Liu “Playing Lottery Tickets with Vision and Language”, Proc. American Association of Artificial Intelligence (<strong>AAAI</strong>), 2022. 
<a style=" color: blue;" href="https://arxiv.org/pdf/2104.11832.pdf">PDF</a> /
<a style=" color: blue;" href="Papers/VLP_lottery_tickets_slides.pdf">Slides</a>
(<font color="#CC5500"><strong>Oral</strong></font>)
</li>

<li> Jinghui Chen, Yu Cheng, <strong>Zhe Gan</strong>, Quanquan Gu and Jingjing Liu “Efficient Robust Training via Backward Smoothing”, Proc. American Association of Artificial Intelligence (<strong>AAAI</strong>), 2022.
<a style=" color: blue;" href="https://arxiv.org/pdf/2010.01278.pdf">PDF</a> 
</li>

<li> Tsu-Jui Fu, Linjie Li, <strong>Zhe Gan</strong>, Kevin Lin, William Yang Wang, Lijuan Wang and Zicheng Liu “VIOLET: End-to-End Video-Language Transformers with Masked Visual-token Modeling”, arXiv preprint, 2022. <a style=" color: blue;" href="https://arxiv.org/pdf/2111.12681.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/tsujuifu/pytorch_violet">Code</a> 
</li>

<li> Yixin Nie*, Linjie Li*, <strong>Zhe Gan</strong>, Shuohang Wang, Chenguang Zhu, Michael Zeng, Zicheng Liu, Mohit Bansal and Lijuan Wang “MLP Architectures for Vision-and-Language Modeling: An Empirical Study”, arXiv preprint, 2022. <a style=" color: blue;" href="https://arxiv.org/pdf/2112.04453.pdf">PDF</a>
</li>

<li> Jianfeng Wang, Xiaowei Hu, <strong>Zhe Gan</strong>, Zhengyuan Yang, Xiyang Dai, Zicheng Liu, Yumao Lu and Lijuan Wang “UFO: A UniFied TransfOrmer for Vision-Language Representation Learning”, arXiv preprint, 2022. <a style=" color: blue;" href="https://arxiv.org/pdf/2111.10023.pdf">PDF</a>
</li>

</ul>

<ul class="articlesubtitle" style="margin-top: -0.25in; margin-bottom: -0.01in;">
<strong>2021</strong>
</ul>

<ul class="articlesubtitle" style="margin-left: 0.2in;">
<li> Tianlong Chen, Yu Cheng, <strong>Zhe Gan</strong>, Lu Yuan, Lei Zhang and Zhangyang Wang “Chasing Sparsity in Vision Transformers: An End-to-End Exploration”, Neural Information Processing Systems (<strong>NeurIPS</strong>), 2021. <a style=" color: blue;" href="https://arxiv.org/pdf/2106.04533.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/VITA-Group/SViTE">Code</a> 
</li>

<li> Xiaohan Chen, Yu Cheng, Shuohang Wang, <strong>Zhe Gan</strong>, Jingjing Liu and Zhangyang Wang “The Elastic Lottery Ticket Hypothesis”, Neural Information Processing Systems (<strong>NeurIPS</strong>), 2021. <a style=" color: blue;" href="https://arxiv.org/pdf/2103.16547.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/VITA-Group/ElasticLTH">Code</a> 
</li>

<li> Tianlong Chen, Yu Cheng, <strong>Zhe Gan</strong>, Jingjing Liu and Zhangyang Wang “Data-Efficient GAN Training Beyond (Just) Augmentations: A Lottery Ticket Perspective”, Neural Information Processing Systems (<strong>NeurIPS</strong>), 2021. <a style=" color: blue;" href="https://arxiv.org/pdf/2103.00397.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/VITA-Group/Ultra-Data-Efficient-GAN-Training">Code</a> 
</li>

<li> Boxin Wang*, Chejian Xu*, Shuohang Wang, <strong>Zhe Gan</strong>, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah and Bo Li “Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models”, Neural Information Processing Systems (<strong>NeurIPS</strong>), Datasets and Benchmarks Track, 2021. <a style=" color: blue;" href="https://arxiv.org/pdf/2111.02840.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://adversarialglue.github.io./">Benchmark and Leaderboard</a> 
(<font color="#FF0000"><strong>Oral</strong></font>)
</li>

<li> Linjie Li*, Jie Lei*, <strong>Zhe Gan</strong>, Licheng Yu, Yen-Chun Chen, Rohit Pillai, Yu Cheng, Luowei Zhou, Xin Eric Wang, William Yang Wang, Tamara Lee Berg, Mohit Bansal, Jingjing Liu, Lijuan Wang and Zicheng Liu “VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation”, Neural Information Processing Systems (<strong>NeurIPS</strong>), Datasets and Benchmarks Track, 2021. <a style=" color: blue;" href="https://arxiv.org/pdf/2106.04632.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/VALUE-Leaderboard/StarterCode">Starter Code</a> /
<a style=" color: #CC5500;" href="https://value-benchmark.github.io/">Leaderboard and Challenge</a> 
</li>

<li> Junya Chen, <strong>Zhe Gan</strong>, Xuan Li, Qing Guo, Liqun Chen, Shuyang Gao, Tagyoung Chung, Yi Xu, Belinda Zeng, Wenlian Lu, Fan Li, Lawrence Carin and Chenyang Tao “Simpler, Faster, Stronger: Breaking The log-K Curse On Contrastive Learners With FlatNCE”, Neural Information Processing Systems (<strong>NeurIPS</strong>), Workshop on Self-Supervised Learning, 2021. <a style=" color: blue;" href="https://arxiv.org/pdf/2107.01152.pdf">PDF</a>
</li>

<li> Linjie Li, Jie Lei, <strong>Zhe Gan</strong> and Jingjing Liu “Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models”, Int. Conf. on Computer Vision (<strong>ICCV</strong>), 2021. <a style=" color: blue;" href="https://arxiv.org/pdf/2106.00245.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://adversarialvqa.github.io/">Dataset</a>
(<font color="#FF0000"><strong>Oral, Top 3% among all submissions</strong></font>)
</li>

<li> Chen Zhu, Yu Cheng, <strong>Zhe Gan</strong>, Furong Huang, Jingjing Liu and Tom Goldstein “MaxVA: Fast Adaptation of Stepsizes by Maximizing Observed Variance of Gradients”, European Conf. Machine Learning (<strong>ECML</strong>), 2021.
<a style=" color: blue;" href="https://arxiv.org/pdf/2006.11918.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/zhuchen03/MaxVA">Code</a>
</li>

<li> Xiaohan Chen, Yu Cheng, Shuohang Wang, <strong>Zhe Gan</strong>, Zhangyang Wang and Jingjing Liu “EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets”, Association for Computational Linguistics (<strong>ACL</strong>), 2021. 
<a style=" color: blue;" href="https://arxiv.org/pdf/2101.00063.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/VITA-Group/EarlyBERT">Code</a>
(<font color="#FF0000"><strong>Oral</strong></font>)
</li>

<li> Shuohang Wang, Luowei Zhou, <strong>Zhe Gan</strong>, Yen-Chun Chen, Yuwei Fang, Siqi Sun, Yu Cheng and Jingjing Liu “Cluster-Former: Clustering-based Sparse Transformer for Question Answering”, Findings of Association for Computational Linguistics (<strong>Findings of ACL</strong>), 2021. 
<a style=" color: blue;" href="https://arxiv.org/pdf/2009.06097.pdf">PDF</a> 
(<font color="#CC5500"><strong>Leaderboard #1 on NaturalQuestions as of Sep. 27, 2020</strong></font>)
</li>

<li> Jie Lei*, Linjie Li*, Luowei Zhou, <strong>Zhe Gan</strong>, Tamara L. Berg, Mohit Bansal and Jingjing Liu “Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling”, Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2021.
<a style=" color: blue;" href="https://arxiv.org/pdf/2102.06183.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/jayleicn/ClipBERT">Code</a>
(<font color="#FF0000"><strong>Oral with 3 Strong Accepts, Best Student Paper Honorable Mention</strong></font>)
</li>

<li> Liqun Chen*, Dong Wang*, <strong>Zhe Gan</strong>, Jingjing Liu, Ricardo Henao and Lawrence Carin “Wasserstein Contrastive Representation Distillation”, Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2021.
<a style=" color: blue;" href="https://arxiv.org/pdf/2012.08674.pdf">PDF</a> 
</li>

<li> Shuyang Dai, <strong>Zhe Gan</strong>, Yu Cheng, Chenyang Tao, Lawrence Carin and Jingjing Liu “APo-VAE: Text Generation in Hyperbolic Space”, North American Chapter of the Association for Computational Linguistics (<strong>NAACL</strong>), 2021.
<a style=" color: blue;" href="https://arxiv.org/pdf/2005.00054.pdf">PDF</a>
</li>

<li> Boxin Wang, Shuohang Wang, Yu Cheng, <strong>Zhe Gan</strong>, Ruoxi Jia, Bo Li and Jingjing Liu “InfoBERT: Improving Robustness of Language Models from An Information Theoretic Perspective”, Int. Conf. Learning Representations (<strong>ICLR</strong>), 2021.
<a style=" color: blue;" href="https://arxiv.org/pdf/2010.02329.pdf">PDF</a> /
<a style=" color: blue;" href="Papers/InfoBERT_slides.pdf">Slides</a> /
<a style=" color: blue;" href="Papers/InfoBERT_poster.pdf">Poster</a> /
<a style=" color: #CC5500;" href="https://github.com/AI-secure/InfoBERT">Code</a>
(<font color="#CC5500"><strong>Leaderboard #1 on Adversarial NLI as of Oct. 9, 2020</strong></font>)
</li>

<li> Siyang Yuan*, Pengyu Cheng*, Ruiyi Zhang, Weituo Hao, <strong>Zhe Gan</strong> and Lawrence Carin “Improving Zero-Shot Voice Style Transfer via Disentangled Representation Learning”, Int. Conf. Learning Representations (<strong>ICLR</strong>), 2021.
<a style=" color: blue;" href="https://arxiv.org/pdf/2103.09420.pdf">PDF</a>
</li>

<li> Yuwei Fang*, Shuohang Wang*, <strong>Zhe Gan</strong>, Siqi Sun and Jingjing Liu “FILTER: An Enhanced Fusion Method for Cross-lingual Language Understanding”, Proc. American Association of Artificial Intelligence (<strong>AAAI</strong>), 2021.
<a style=" color: blue;" href="https://arxiv.org/pdf/2009.05166.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/yuwfan/FILTER">Code</a> /
<a style=" color: blue;" href="Papers/filter_slides.pdf">Slides</a> /
<a style=" color: blue;" href="https://medium.com/@conversationalaiteam/filter-understand-foreign-languages-better-4bfa6d12377f">Blog</a> 
(<font color="#CC5500"><strong>Leaderboard #1 on XTREME and XGLUE as of Sep. 8, 2020</strong></font>)
</li>

<li> Wenhu Chen, <strong>Zhe Gan</strong>, Linjie Li, Yu Cheng, William Wang and Jingjing Liu “Meta Module Network for Compositional Visual Reasoning”, Winter Conf. on Applications of Computer Vision (<strong>WACV</strong>), 2021.
<a style=" color: blue;" href="https://arxiv.org/pdf/1910.03230.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/wenhuchen/Meta-Module-Network">Code</a>
(<font color="#FF0000"><strong>Best Student Paper Honorable Mention</strong></font>)
</li>

<li> Luowei Zhou, Jingjing Liu, Yu Cheng, <strong>Zhe Gan</strong> and Lei Zhang “CUPID: Adaptive Curation of Pre-training Data for Video-and-Language Representation Learning”, arXiv preprint, 2021. <a style=" color: blue;" href="https://arxiv.org/pdf/2104.00285.pdf">PDF</a>
</li>

<li> Linjie Li, <strong>Zhe Gan</strong> and Jingjing Liu “A Closer Look at the Robustness of Vision-and-Language Pre-trained Models”, arXiv preprint, 2021.
<a style=" color: blue;" href="https://arxiv.org/pdf/2012.08673.pdf">PDF</a> /
<a style=" color: blue;" href="Papers/MANGO_talk.pdf">Slides</a>
(<font color="#CC5500"><strong>SOTA on 7 VQA robustness benchmarks as of April 23, 2021</strong></font>)
</li>

<li> Yuwei Fang, Shuohang Wang, <strong>Zhe Gan</strong>, Siqi Sun, Jingjing Liu and Chenguang Zhu “Accelerating Real-Time Question Answering via Question Generation”, arXiv preprint, 2021.
<a style=" color: blue;" href="https://arxiv.org/pdf/2009.05167.pdf">PDF</a> 
</li>

<li> Dong Wang, Yuewei Yang, Chenyang Tao, <strong>Zhe Gan</strong>, Liqun Chen, Fanjie Kong, Ricardo Henao and Lawrence Carin “Proactive Pseudo-Intervention: Contrastive Learning For Interpretable Vision Models”, arXiv preprint, 2021.
<a style=" color: blue;" href="https://arxiv.org/pdf/2012.03369.pdf">PDF</a> 
</li>

<li> Minhao Cheng, <strong>Zhe Gan</strong>, Yu Cheng, Shuohang Wang, Cho-Jui Hsieh and Jingjing Liu “Adversarial Masking: Towards Understanding Robustness Trade-off for Generalization”, OpenReview, 2021. <a style=" color: blue;" href="https://openreview.net/pdf?id=LNtTXJ9XXr">PDF</a>
</li>

</ul>

<ul class="articlesubtitle" style="margin-top: -0.25in; margin-bottom: -0.01in;">
<strong>2020</strong>
</ul>

<ul class="articlesubtitle" style="margin-left: 0.2in;">
<li> <strong>Zhe Gan</strong>, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng and Jingjing Liu “Large-Scale Adversarial Training for Vision-and-Language Representation Learning”, Neural Information Processing Systems (<strong>NeurIPS</strong>), 2020.
<a style=" color: blue;" href="https://arxiv.org/pdf/2006.06195.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/zhegan27/VILLA">Code-I</a> /
<a style=" color: #CC5500;" href="https://github.com/zhegan27/LXMERT-AdvTrain">Code-II</a> /
<a style=" color: blue;" href="Papers/villa_slides.pdf">Slides</a> /
<a style=" color: blue;" href="Papers/villa_poster.pdf">Poster</a> /
<a style=" color: blue;" href="https://towardsdatascience.com/villa-a-generic-adversarial-training-technique-for-vision-and-language-5e6cd5d3abd7">Blog</a>
(<font color="#FF0000"><strong>Spotlight</strong></font>)
<font color="#CC5500"><strong>Top 4% among all submissions, SOTA on 6 Vision+Language tasks</strong></font>
</li>

<li> Siqi Sun, <strong>Zhe Gan</strong>, Yu Cheng, Yuwei Fang, Shuohang Wang and Jingjing Liu “Contrastive Distillation on Intermediate Representations for Language Model Compression”, Conf. on Empirical Methods in Natural Language Processing (<strong>EMNLP</strong>), 2020. 
<a style=" color: blue;" href="https://arxiv.org/pdf/2009.14167.pdf">PDF</a> /
<a style=" color: blue;" href="https://towardsdatascience.com/codir-train-smaller-faster-nlp-models-bac6318a9950">Blog</a> /
<a style=" color: #CC5500;" href="https://github.com/intersun/CoDIR">Code</a> 
</li>

<li> Shuohang Wang, Yuwei Fang, Siqi Sun, <strong>Zhe Gan</strong>, Yu Cheng, Jing Jiang and Jingjing Liu  “Cross-Thought for Sentence Encoder Pre-training”, Conf. on Empirical Methods in Natural Language Processing (<strong>EMNLP</strong>), 2020. 
<a style=" color: blue;" href="https://arxiv.org/pdf/2010.03652.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/shuohangwang/Cross-Thought">Code</a> 
</li>

<li> Yue Dong, Shuohang Wang, <strong>Zhe Gan</strong>, Yu Cheng, Jackie Chi Kit Cheung and Jingjing Liu “Multi-Fact Correction in Abstractive Text Summarization”, Conf. on Empirical Methods in Natural Language Processing (<strong>EMNLP</strong>), 2020. 
<a style=" color: blue;" href="https://arxiv.org/pdf/2010.02443.pdf">PDF</a> /
<a style=" color: blue;" href="Papers/SpanFact_slides.pdf">Slides</a> /
<a style=" color: blue;" href="https://conversationalaiteam.medium.com/spanfact-fix-your-factually-incorrect-summaries-459481978f86">Blog</a>
</li>

<li> Linjie Li*, Yen-Chun Chen*, Yu Cheng, <strong>Zhe Gan</strong>, Licheng Yu and Jingjing Liu “HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training”, Conf. on Empirical Methods in Natural Language Processing (<strong>EMNLP</strong>), 2020. 
<a style=" color: blue;" href="https://arxiv.org/pdf/2005.00200.pdf">PDF</a> /
<a style=" color: blue;" href="Papers/HERO_talk.pdf">Slides</a> /
<a style=" color: #CC5500;" href="https://github.com/linjieli222/HERO">Code</a> /
<a style=" color: blue;" href="https://medium.com/@conversationalaiteam/hero-youll-never-have-to-watch-long-videos-again-ee0d5e2ba4fd">Blog</a>
(<font color="#CC5500"><strong>SOTA on 8 Video+Language datasets, Leaderboard #1 on TVR and TVC as of Sep. 15, 2020</strong></font>)
</li>

<li> Yizhe Zhang*, Guoyin Wang*, Chunyuan Li, <strong>Zhe Gan</strong>, Chris Brockett and Bill Dolan “POINTER: Constrained Progressive Text Generation via Insertion-based Generative Pre-training”, Conf. on Empirical Methods in Natural Language Processing (<strong>EMNLP</strong>), 2020. 
<a style=" color: blue;" href="https://arxiv.org/pdf/2005.00558.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/dreasysnail/POINTER">Code</a> /
<a style=" color: #CC5500;" href="http://52.247.25.3:8900/">Demo</a>
</li>

<li> Yuwei Fang, Siqi Sun, <strong>Zhe Gan</strong>, Rohit Pillai, Shuohang Wang and Jingjing Liu “Hierarchical Graph Network for Multi-hop Question Answering”, Conf. on Empirical Methods in Natural Language Processing (<strong>EMNLP</strong>), 2020.
<a style=" color: blue;" href="https://arxiv.org/pdf/1911.03631.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/yuwfan/HGN">Code</a>
(<font color="#CC5500"><strong>Leaderboard #1 on HotpotQA as of Dec. 1st, 2019</strong></font>)
</li>

<li> Yu Cheng, <strong>Zhe Gan</strong>, Yizhe Zhang, Oussama Elachqar, Dianqi Li and Jingjing Liu “Contextual Text Style Transfer”, Findings of Empirical Methods in Natural Language Processing (<strong>Findings of EMNLP</strong>), 2020. 
<a style=" color: blue;" href="https://arxiv.org/pdf/2005.00136.pdf">PDF</a>
</li>

<li> Yi Wei, <strong>Zhe Gan</strong>, Wenbo Li,  Siwei Lyu, Ming-Ching Chang, Lei Zhang, Jianfeng Gao and Pengchuan Zhang “MagGAN: High-Resolution Face Attribute Editing with Mask-Guided Generative Adversarial Network”, Asian Conf. on Computer Vision (<strong>ACCV</strong>), 2020.
<a style=" color: blue;" href="https://arxiv.org/pdf/2010.01424.pdf">PDF</a>
</li>

<li> Shuyang Dai, Yu Cheng, Yizhe Zhang, <strong>Zhe Gan</strong>, Jingjing Liu and Lawrence Carin “Contrastively Smoothed Class Alignment for Unsupervised Domain Adaptation”, Asian Conf. on Computer Vision (<strong>ACCV</strong>), 2020.
<a style=" color: blue;" href="https://arxiv.org/pdf/1909.05288.pdf">PDF</a>
</li>

<li> Jize Cao, <strong>Zhe Gan</strong>, Yu Cheng, Licheng Yu, Yen-Chun Chen and Jingjing Liu “Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models”, European Conf. on Computer Vision (<strong>ECCV</strong>), 2020.
<a style=" color: blue;" href="https://arxiv.org/pdf/2005.07310.pdf">PDF</a>
(<font color="#FF0000"><strong>Spotlight</strong></font>)
<font color="#CC5500"><strong>Top 5% among all submissions</strong></font>
</li>

<li> Yen-Chun Chen*, Linjie Li*, Licheng Yu*, Ahmed El Kholy, Faisal Ahmed, <strong>Zhe Gan</strong>, Yu Cheng and Jingjing Liu “UNITER: UNiversal Image-TExt Representation Learning”, European Conf. on Computer Vision (<strong>ECCV</strong>), 2020.
<a style=" color: blue;" href="https://arxiv.org/pdf/1909.11740.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/ChenRocks/UNITER">Code</a>
(<font color="#CC5500"><strong>SOTA on 13 Vision+Language Datasets/Tasks, No. 1 on VCR and NLVR2 leaderboards as of Sep. 2019</strong></font>)
</li>

<li> Yu Cheng, <strong>Zhe Gan</strong>, Yitong Li, Jingjing Liu and Jianfeng Gao “Sequential Attention GAN for Interactive Image Editing”, ACM International Conference on Multimedia (<strong>ACMMM</strong>), 2020.
<a style=" color: blue;" href="https://arxiv.org/pdf/1812.08352.pdf">PDF</a>
</li>

<li> Pengyu Cheng, Weituo Hao, Shuyang Dai, Jiachang Liu, <strong>Zhe Gan</strong> and Lawrence Carin “CLUB: A Contrastive Log-ratio Upper Bound of Mutual Information”, Int. Conf. Machine Learning (<strong>ICML</strong>), 2020.
<a style=" color: blue;" href="https://arxiv.org/pdf/2006.12013.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/Linear95/CLUB">Code</a> 
</li>

<li> Liqun Chen, <strong>Zhe Gan</strong>, Yu Cheng, Linjie Li, Lawrence Carin and Jingjing Liu “Graph Optimal Transport for Cross-Domain Alignment”, Int. Conf. Machine Learning (<strong>ICML</strong>), 2020. 
<a style=" color: blue;" href="https://arxiv.org/pdf/2006.14744.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/LiqunChen0606/Graph-Optimal-Transport">Code</a>
</li>

<li> Jiacheng Xu, <strong>Zhe Gan</strong>, Yu Cheng and Jingjing Liu “Discourse-Aware Neural Extractive Text Summarization”, Association for Computational Linguistics (<strong>ACL</strong>), 2020. 
<a style=" color: blue;" href="https://arxiv.org/pdf/1910.14142.pdf">PDF</a> /
<a style=" color: blue;" href="https://medium.com/@conversationalaiteam/discobert-a-bert-that-shortens-your-reading-time-be49d03e1ff">Blog</a> /
<a style=" color: #CC5500;" href="https://github.com/jiacheng-xu/DiscoBERT">Code</a> 
</li>

<li> Yen-Chun Chen, <strong>Zhe Gan</strong>, Yu Cheng, Jingzhou Liu and Jingjing Liu “Distilling Knowledge Learned in BERT for Text Generation”, Association for Computational Linguistics (<strong>ACL</strong>), 2020.
<a style=" color: blue;" href="https://arxiv.org/pdf/1911.03829.pdf">PDF</a> /
<a style=" color: blue;" href="Papers/DistillBertTextgen_ACL2020.pdf">Slides</a> /
<a style=" color: blue;" href="https://medium.com/@conversationalaiteam/distill-bert-using-bert-for-smarter-text-generation-aa65ba6facf0">Blog</a> /
<a style=" color: #CC5500;" href="https://github.com/ChenRocks/Distill-BERT-Textgen">Code</a> 
</li>

<li> Ruiyi Zhang, Changyou Chen, <strong>Zhe Gan</strong>, Wenlin Wang, Dinghan Shen, Guoyin Wang, Zheng Wen and Lawrence Carin “Improving Adversarial Text Generation by Modeling the Distant Future”, Association for Computational Linguistics (<strong>ACL</strong>), 2020. 
<a style=" color: blue;" href="https://arxiv.org/pdf/2005.01279.pdf">PDF</a> /
<a style=" color: blue;" href="https://arxiv.org/pdf/1811.00696.pdf">Old</a>
</li>

<li> Yandong Li, Yu Cheng, <strong>Zhe Gan</strong>, Licheng Yu, Liqiang Wang and Jingjing Liu “BachGAN: High-Resolution Image Synthesis from Salient Object Layout”, Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2020.
<a style=" color: blue;" href="https://arxiv.org/pdf/2003.11690.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/Cold-Winter/BachGAN">Code</a> 
</li>

<li> Jingzhou Liu, Wenhu Chen, Yu Cheng, <strong>Zhe Gan</strong>, Licheng Yu, Yiming Yang and Jingjing Liu “VIOLIN: A Large-Scale Dataset for Video-and-Language Inference”, Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2020.
<a style=" color: blue;" href="https://arxiv.org/pdf/2003.11618.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/jimmy646/violin">Code</a> 
</li>

<li> Ruiyi Zhang, Changyou Chen, <strong>Zhe Gan</strong>, Zheng Wen, Wenlin Wang and Lawrence Carin “Nested-Wasserstein Self-Imitation Learning for Sequence Generation”, Artificial Intelligence and Statistics (<strong>AISTATS</strong>), 2020.
<a style=" color: blue;" href="https://arxiv.org/pdf/2001.06944.pdf">PDF</a> /
<a style=" color: blue;" href="http://bayesiandeeplearning.org/2019/papers/7.pdf">Workshop on Bayesian Deep Learning, NeurIPS 2019</a>
</li>

<li> Chen Zhu, Yu Cheng, <strong>Zhe Gan</strong>, Siqi Sun, Tom Goldstein and Jingjing Liu “FreeLB: Enhanced Adversarial Training for Natural Language Understanding”, Int. Conf. Learning Representations (<strong>ICLR</strong>), 2020.
<a style=" color: blue;" href="https://arxiv.org/pdf/1909.11764.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/zhuchen03/FreeLB">Code</a> 
(<font color="#FF0000"><strong>Spotlight</strong></font>)
<font color="#CC5500"><strong>Leaderboard #1 on GLUE, ARC Easy/Challenge and Commonsense QA as of Sep. 2019</strong></font>
</li>

<li> Wenlin Wang, Hongteng Xu, <strong>Zhe Gan</strong>, Bai Li, Guoyin Wang, Liqun Chen, Qian Yang, Wenqi Wang and Lawrence Carin “Graph-Driven Generative Models for Heterogeneous Multi-Task Learning”, Proc. American Association of Artificial Intelligence (<strong>AAAI</strong>), 2020.
<a style=" color: blue;" href="https://arxiv.org/pdf/1911.08709.pdf">PDF</a> /
<a style=" color: blue;" href="Papers/GDVAE_poster.pdf">Poster</a> /
<a style=" color: blue;" href="Papers/GDVAE_slides.pdf">Slides</a>
(<font color="#FF0000"><strong>Spotlight</strong></font>) 
</li>

<li> Junjie Hu, Yu Cheng, <strong>Zhe Gan</strong>, Jingjing Liu, Jianfeng Gao and Graham Neubig “What Makes A Good Story? Designing Composite Rewards for Visual Storytelling”, Proc. American Association of Artificial Intelligence (<strong>AAAI</strong>), 2020.
<a style=" color: blue;" href="https://arxiv.org/pdf/1909.05316.pdf">PDF</a> /
<a style=" color: blue;" href="Papers/aaai20hu-poster.pdf">Poster</a> /
<a style=" color: #CC5500;" href="https://github.com/JunjieHu/ReCo-RL">Code</a> 
(<font color="#FF0000"><strong>Spotlight</strong></font>)
</li>

</ul>

<ul class="articlesubtitle" style="margin-top: -0.25in; margin-bottom: -0.01in;">
<strong>2019</strong>
</ul>
				
<ul class="articlesubtitle" style="margin-left: 0.2in;">
<li> Wenlin Wang, Chenyang Tao, <strong>Zhe Gan</strong>, Guoyin Wang, Liqun Chen, Xinyuan Zhang, Ruiyi Zhang, Qian Yang, Ricardo Henao and Lawrence Carin “Improving Textual Network Learning with Variational Homophilic Embeddings”, Neural Information Processing Systems (<strong>NeurIPS</strong>), 2019.
<a style=" color: blue;" href="https://arxiv.org/pdf/1909.13456.pdf">PDF</a> 
</li>

<li> Siqi Sun, Yu Cheng, <strong>Zhe Gan</strong> and Jingjing Liu “Patient Knowledge Distillation for BERT Model Compression”, Conf. on Empirical Methods in Natural Language Processing (<strong>EMNLP</strong>), 2019. 
<a style=" color: blue;" href="https://arxiv.org/pdf/1908.09355.pdf">PDF</a> /
<a style=" color: blue;" href="Papers/PatientTeacher_EMNLP2019_poster.pdf">Poster</a> /
<a style=" color: #CC5500;" href="https://github.com/intersun/PKD-for-BERT-Model-Compression">Code</a>
</li>


<li> Huazheng Wang, <strong>Zhe Gan</strong>, Xiaodong Liu, Jingjing Liu, Jianfeng Gao and Hongning Wang “Adversarial Domain Adaptation for Machine Reading Comprehension”, Conf. on Empirical Methods in Natural Language Processing (<strong>EMNLP</strong>), 2019. 
<a style=" color: blue;" href="https://arxiv.org/pdf/1908.09209.pdf">PDF</a> /
<a style=" color: blue;" href="Papers/AdaMRC_EMNLP2019_poster.pdf">Poster</a> /
<a style=" color: #CC5500;" href="https://github.com/huazhengwang/AdaMRC">Code</a> 
</li>

<li> Dianqi Li, Yizhe Zhang, <strong>Zhe Gan</strong>, Yu Cheng, Chris Brockett, Ming-Ting Sun and Bill Dolan  “Domain Adaptive Text Style Transfer”, Conf. on Empirical Methods in Natural Language Processing (<strong>EMNLP</strong>), 2019. 
<a style=" color: blue;" href="https://arxiv.org/pdf/1908.09395.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/cookielee77/DAST">Code</a> 
</li>

<li> Ming Jiang, Qiuyuan Huang, Lei Zhang, Xin Wang, Pengchuan Zhang, <strong>Zhe Gan</strong>, Jana Diesner and Jianfeng Gao “TIGEr: Text-to-Image Grounding for Image Caption Evaluation”, Conf. on Empirical Methods in Natural Language Processing (<strong>EMNLP</strong>), 2019. 
<a style=" color: blue;" href="https://arxiv.org/pdf/1909.02050.pdf">PDF</a> /
<a style=" color: blue;" href="Papers/TIGEr_EMNLP2019_poster.pdf">Poster</a> /
<a style=" color: #CC5500;" href="https://github.com/SeleenaJM/CapEval">Code</a>
</li>

<li> Linjie Li, <strong>Zhe Gan</strong>, Yu Cheng and Jingjing Liu “Relation-Aware Graph Attention Network for Visual Question Answering”, Int. Conf. on Computer Vision (<strong>ICCV</strong>), 2019. 
<a style=" color: blue;" href="https://arxiv.org/pdf/1903.12314.pdf">PDF</a> /
<a style=" color: blue;" href="Papers/ReGAT_supp_ICCV_2019.pdf">Supp</a> /
<a style=" color: blue;" href="Papers/ReGAT_poster_ICCV_2019.pdf">Poster</a> /
<a style=" color: #CC5500;" href="https://github.com/linjieli222/VQA_ReGAT">Code</a>
</li>

<li> <strong>Zhe Gan</strong>, Yu Cheng, Ahmed El Kholy, Linjie Li, Jingjing Liu and Jianfeng Gao “Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog”, Association for Computational Linguistics (<strong>ACL</strong>), 2019. 
<a style=" color: blue;" href="https://arxiv.org/pdf/1902.00579.pdf">PDF</a> /
<a style=" color: blue;" href="Papers/ReDAN_poster.pdf">Poster</a>
</li>

<li> Liyiming Ke, Xiujun Li, Yonatan Bisk, Ari Holtzman, <strong>Zhe Gan</strong>, Jingjing Liu, Jianfeng Gao, Yejin Choi and Siddhartha Srinivasa “Tactical Rewind: Self-Correction via Backtracking in Vision-and-Language Navigation”, Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2019.
<a style=" color: blue;" href="https://arxiv.org/pdf/1903.02547.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://www.youtube.com/watch?v=ik9uz06Fcpk&feature=youtu.be">YouTube</a> /
<a style=" color: #CC5500;" href="https://github.com/Kelym/FAST">Code</a>
(<font color="#FF0000"><strong>Oral</strong></font>)
</li>

<li> Yitong Li, <strong>Zhe Gan</strong>, Yelong Shen, Jingjing Liu, Yu Cheng,  Yuexin Wu, Lawrence Carin, David Carlson and Jianfeng Gao “StoryGAN: A Sequential Conditional GAN for Story Visualization”, Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2019.
<a style=" color: blue;" href="https://arxiv.org/pdf/1812.02784.pdf">PDF</a> /
<a style=" color: blue;" href="Papers/StoryGAN_slides.pdf">Slides</a> /
<a style=" color: blue;" href="Papers/StoryGAN_poster.pdf">Poster</a> /
<a style=" color: #CC5500;" href="https://github.com/yitong91/StoryGAN">Code</a>
</li>

<li> Wenlin Wang, <strong>Zhe Gan</strong>, Hongteng Xu, Ruiyi Zhang, Guoyin Wang, Dinghan Shen, Changyou Chen and Lawrence Carin “Topic-Guided Variational Autoencoders for Text Generation”, North American Chapter of the Association for Computational Linguistics (<strong>NAACL</strong>), 2019.
<a style=" color: blue;" href="https://arxiv.org/pdf/1903.07137.pdf">PDF</a> 
(<font color="#FF0000"><strong>Oral</strong></font>)
</li>

<li> Liqun Chen, Yizhe Zhang, Ruiyi Zhang, Chenyang Tao, <strong>Zhe Gan</strong>, Haichao Zhang, Bai Li, Dinghan Shen, Changyou Chen and Lawrence Carin “Improving Sequence-to-Sequence Learning via Optimal Transport”, Int. Conf. Learning Representations (<strong>ICLR</strong>), 2019.
<a style=" color: blue;" href="https://arxiv.org/pdf/1901.06283.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/LiqunChen0606/OT-Seq2Seq">Code</a>
</li>

<li> Qiuyuan Huang*, <strong>Zhe Gan</strong>*, Asli Celikyilmaz, Dapeng Wu, Jianfeng Wang and Xiaodong He “Hierarchically Structured Reinforcement Learning for Topically Coherent Visual Story Generation”, Proc. American Association of Artificial Intelligence (<strong>AAAI</strong>), 2019.
<a style=" color: blue;" href="https://arxiv.org/pdf/1805.08191.pdf">PDF</a>
(<font color="#FF0000"><strong>Spotlight</strong></font>)
</li>

</ul>

<ul class="articlesubtitle" style="margin-top: -0.25in; margin-bottom: -0.01in;">
<strong>2018</strong>
</ul>

<ul class="articlesubtitle" style="margin-left: 0.2in;">
<li> Yizhe Zhang, Michel Galley, Jianfeng Gao, <strong>Zhe Gan</strong>, Xiujun Li, Chris Brockett and Bill Dolan “Generating Informative and Diverse Conversational Responses via Adversarial Information Maximization”, Neural Information Processing Systems (<strong>NeurIPS</strong>), 2018.
<a style=" color: blue;" href="https://arxiv.org/pdf/1809.05972.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://www.microsoft.com/en-us/research/blog/getting-into-a-conversational-groove-new-approach-encourages-risk-taking-in-data-driven-neural-modeling/">Blog</a> /
<a style=" color: #CC5500;" href="https://github.com/dreasysnail/converse_GAN">Code</a>
</li>

<li> Liqun Chen, Shuyang Dai, Chenyang Tao, Dinghan Shen, <strong>Zhe Gan</strong>, Haichao Zhang, Yizhe Zhang and Lawrence Carin “Adversarial Text Generation via Feature-Mover's Distance”, Neural Information Processing Systems (<strong>NeurIPS</strong>), 2018.
<a style=" color: blue;" href="https://arxiv.org/pdf/1809.06297.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/LiqunChen0606/FM-GAN">Code</a>
</li>

<li> Xinyuan Zhang, Ricardo Henao, <strong>Zhe Gan</strong>, Yitong Li and Lawrence Carin “Multi-Label Learning from Medical Plain Text with Convolutional Residual Models”, Machine Learning for Healthcare (<strong>MLHC</strong>), 2018. 
<a style=" color: blue;" href="https://arxiv.org/pdf/1801.05062.pdf">PDF</a>
(<font color="#FF0000"><strong>Spotlight</strong></font>)
</li>

<li> Yunchen Pu, Shuyang Dai, <strong>Zhe Gan</strong>, Weiyao Wang, Guoyin Wang, Yizhe Zhang, Ricardo Henao and Lawrence Carin “JointGAN: Multi-Domain Joint Distribution Learning with Generative Adversarial Nets”, Int. Conf. Machine Learning (<strong>ICML</strong>), 2018. 
<a style=" color: blue;" href="https://arxiv.org/pdf/1806.02978.pdf">PDF</a> /
<a style=" color: blue;" href="Papers/JointGAN_appendix.pdf">Supp</a> /
<a style=" color: blue;" href="Papers/JointGAN_poster.pdf">Poster</a> /
<a style=" color: blue;" href="Papers/JointGAN_slides.pdf">Slides</a> /
<a style=" color: #CC5500;" href="https://github.com/sdai654416/Joint-GAN">Code</a>
</li>

<li> Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, <strong>Zhe Gan</strong>, Xiaolei Huang and Xiaodong He “AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks”, Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2018.
<a style=" color: blue;" href="https://arxiv.org/pdf/1711.10485.pdf">PDF</a> /
<a style=" color: blue;" href="Papers/cvpr18_poster_AttnGAN.pdf">Poster</a> /
<a style=" color: #CC5500;" href="https://github.com/taoxugit/AttnGAN">Code</a> /
<a style=" color: #CC5500;" href="https://blogs.microsoft.com/ai/drawing-ai/">Blog</a> 
</li>

<li> Wenlin Wang, <strong>Zhe Gan</strong>, Wenqi Wang, Dinghan Shen, Jiaji Huang, Wei Ping, Sanjeev Satheesh and Lawrence Carin “Topic Compositional Neural Language Model”, Artificial Intelligence and Statistics (<strong>AISTATS</strong>), 2018. 
<a style=" color: blue;" href="https://arxiv.org/pdf/1712.09783.pdf">PDF</a>
</li>

<li> Yunchen Pu, Martin Renqiang Min, <strong>Zhe Gan</strong> and Lawrence Carin “Adaptive Feature Abstraction for Translating Video to Text”, Proc. American Association of Artificial Intelligence (<strong>AAAI</strong>), 2018.
<a style=" color: blue;" href="https://arxiv.org/pdf/1611.07837.pdf">PDF</a>
</li>

</ul>

<ul class="articlesubtitle" style="margin-top: -0.25in; margin-bottom: -0.01in;">
<strong>2017</strong>
</ul>
				
<ul class="articlesubtitle" style="margin-left: 0.2in;">
<li> <strong>Zhe Gan</strong>*, Liqun Chen*, Weiyao Wang, Yunchen Pu, Yizhe Zhang, Hao Liu, Chunyuan Li and Lawrence Carin “Triangle Generative Adversarial Networks”, Neural Information Processing Systems (<strong>NeurIPS</strong>), 2017.
<a style=" color: blue;" href="https://arxiv.org/pdf/1709.06548.pdf">PDF</a> /
<a style=" color: blue;" href="Papers/TriangleGAN_poster.pdf">Poster</a> /
<a style=" color: #CC5500;" href="https://github.com/LiqunChen0606/Triangle-GAN">Code</a>
</li>

<li> Yunchen Pu, Weiyao Wang, Ricardo Henao, Liqun Chen, <strong>Zhe Gan</strong>,   Chunyuan Li and Lawrence Carin “Adversarial Symmetric Variational Autoencoder”, Neural Information Processing Systems (<strong>NeurIPS</strong>), 2017.
<a style=" color: blue;" href="https://arxiv.org/pdf/1711.04915.pdf">PDF</a>
</li>

<li> Yunchen Pu, <strong>Zhe Gan</strong>, Ricardo Henao, Chunyuan Li, Shaobo Han and Lawrence Carin “VAE Learning via Stein Variational Gradient Descent”, Neural Information Processing Systems (<strong>NeurIPS</strong>), 2017.
<a style=" color: blue;" href="https://arxiv.org/pdf/1704.05155.pdf">PDF</a> 
</li>

<li> Yizhe Zhang, Dinghan Shen, Guoying Wang, <strong>Zhe Gan</strong>, Ricardo Henao and Lawrence Carin “Deconvolutional Paragraph Representation Learning”, Neural Information Processing Systems (<strong>NeurIPS</strong>), 2017.
<a style=" color: blue;" href="https://arxiv.org/pdf/1708.04729.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/dreasysnail/textCNN_public">Code</a>
</li>

<li> <strong>Zhe Gan</strong>, Yunchen Pu, Ricardo Henao, Chunyuan Li, Xiaodong He and Lawrence Carin “Learning Generic Sentence Representations Using Convolutional Neural Networks”, Conf. on Empirical Methods in Natural Language Processing (<strong>EMNLP</strong>), 2017. 
<a style=" color: blue;" href="https://arxiv.org/pdf/1611.07897.pdf">PDF</a> /
<a style=" color: blue;" href="Papers/slides_ConvSent.pdf">Slides</a> /
<a style=" color: #CC5500;" href="https://github.com/zhegan27/ConvSent">Code</a>
(<font color="#FF0000"><strong>Oral</strong></font>)
</li>

<li> Yizhe Zhang, <strong>Zhe Gan</strong>, Kai Fan, Zhi Chen, Ricardo Henao, Dinghan Shen and Lawrence Carin “Adversarial Feature Matching for Text Generation”, Int. Conf. Machine Learning (<strong>ICML</strong>), 2017. 
<a style=" color: blue;" href="https://arxiv.org/pdf/1706.03850.pdf">PDF</a> /
<a style=" color: blue;" href="Papers/textGAN_supp.pdf">Supp</a> /
<a style=" color: blue;" href="Papers/textGAN_slides.pdf">Slides</a> /
<a style=" color: blue;" href="Papers/textGAN_poster.pdf">Poster</a> /
<a style=" color: #CC5500;" href="https://github.com/dreasysnail/textGAN_public">Code</a>
</li>

<li> Yizhe Zhang, Changyou Chen, <strong>Zhe Gan</strong>, Ricardo Henao and Lawrence Carin “Stochastic Gradient Monomial Gamma Sampler”, Int. Conf. Machine Learning (<strong>ICML</strong>), 2017. 
<a style=" color: blue;" href="https://arxiv.org/pdf/1706.01498.pdf">PDF</a> /
<a style=" color: blue;" href="Papers/sgmgs_supp.pdf">Supp</a> /
<a style=" color: blue;" href="Papers/sgmgs_slides.pdf">Slides</a> /
<a style=" color: blue;" href="Papers/sgmgs_poster.pdf">Poster</a> /
<a style=" color: #CC5500;" href="https://github.com/dreasysnail/SGMGT">Code</a>
</li>

<li> <strong>Zhe Gan</strong>*, Chunyuan Li*, Changyou Chen, Yunchen Pu, Qinliang Su and Lawrence Carin “Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling”, Association for Computational Linguistics (<strong>ACL</strong>), 2017. 
<a style=" color: blue;" href="https://arxiv.org/pdf/1611.08034.pdf">PDF</a> /
<a style=" color: blue;" href="Papers/bayesian_rnn_supp.pdf">Supp</a> /
<a style=" color: blue;" href="https://www.dropbox.com/s/4zr7ep5pcl6mktj/slides_BayesianRNN.pdf?dl=0">Slides</a> /
<a style=" color: #CC5500;" href="https://github.com/zhegan27/Bayesian_RNN">Code</a>
(<font color="#FF0000"><strong>Oral</strong></font>)
</li>

<li> <strong>Zhe Gan</strong>, Chuang Gan, Xiaodong He, Yunchen Pu, Kenneth Tran, Jianfeng Gao, Lawrence Carin and Li Deng “Semantic Compositional Networks for Visual Captioning”, Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2017. 
<a style=" color: blue;" href="https://arxiv.org/pdf/1611.08002.pdf">PDF</a> /
<a style=" color: blue;" href="Papers/SCN_slides.pdf">Slides</a> /
<a style=" color: blue;" href="Papers/SCN_slides2.pdf">Slides2</a> /
<a style=" color: blue;" href="Papers/SCN_poster.pdf">Poster</a> /
<a style=" color: blue;" href="Papers/visual_captioning_poster.pdf">Poster2</a> /
<a style=" color: blue;" href="https://www.youtube.com/watch?v=McJchj3gkZA">Video</a> /
<a style=" color: #CC5500;" href="https://github.com/zhegan27/Semantic_Compositional_Nets">Code</a>
(<font color="#FF0000"><strong>Spotlight</strong></font>)
</li>

<li> Chuang Gan, <strong>Zhe Gan</strong>, Xiaodong He, Jianfeng Gao and Li Deng “StyleNet: Generating Attractive Visual Captions with Styles”, Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2017. 
<a style=" color: blue;" href="Papers/StyleNet_CVPR2017.pdf">PDF</a> /
<a style=" color: #CC5500;" href="Papers/FlickrStyle_v0.9.zip">Data</a>
</li>

<li> <strong>Zhe Gan</strong>, P.D. Singh, Ameet Joshi, Xiaodong He, Jianshu Chen, Jianfeng Gao and Li Deng “Character-level Deep Conflation for Business Data Analytics”, Int. Conf. on Acoustics, Speech and Signal Processing (<strong>ICASSP</strong>), 2017. 
<a style=" color: blue;" href="https://arxiv.org/pdf/1702.02640.pdf">PDF</a> /
<a style=" color: blue;" href="Papers/ICASSP_talk.pdf">Slides</a> /
<a style=" color: #CC5500;" href="https://github.com/zhegan27/Deep_Conflation_Model">Code</a>
</li>

<li> Yin Xian, Yunchen Pu, <strong>Zhe Gan</strong>, Liang Lu and Andrew Thompson “Adaptive DCTNet for Audio Signal Classification”, Int. Conf. on Acoustics, Speech and Signal Processing (<strong>ICASSP</strong>), 2017. 
<a style=" color: blue;" href="https://arxiv.org/pdf/1612.04028.pdf">PDF</a> 
</li>

<li> Qinliang Su, Xuejun Liao, Chunyuan Li, <strong>Zhe Gan</strong> and Lawrence Carin “Unsupervised Learning with Truncated Gaussian Graphical Models”, Proc. American Association of Artificial Intelligence (<strong>AAAI</strong>), 2017. 
<a style=" color: blue;" href="https://arxiv.org/pdf/1611.04920.pdf">PDF</a> 
(<font color="#FF0000"><strong>Oral</strong></font>)
</li>

</ul>

<ul class="articlesubtitle" style="margin-top: -0.25in; margin-bottom: -0.01in;">
<strong>2016</strong>
</ul>
				
<ul class="articlesubtitle" style="margin-left: 0.2in;">
<li> Yizhe Zhang, <strong>Zhe Gan</strong> and Lawrence Carin “Generating Text via Adversarial Training”, Workshop on Adversarial Training, NeurIPS 2016. 
<a style=" color: blue;" href="Papers/textGAN_nips2016_workshop.pdf">PDF</a> /
<a style=" color: #CC5500;" href="https://github.com/dreasysnail/textGAN_public">Code</a>
</li>


<li> Yin Xian, Yunchen Pu, <strong>Zhe Gan</strong>, Liang Lu and Andrew Thompson “Modified DCTNet for Audio Signals Classification”, Journal of the Acoustical Society of America, 2016. 
<a style=" color: blue;" href="http://asa.scitation.org/doi/abs/10.1121/1.4970932">Link</a> 
</li>

<li> Yunchen Pu, <strong>Zhe Gan</strong>, Ricardo Henao, Xin Yuan, Chunyuan Li, Andrew Stevens and Lawrence Carin “Variational Autoencoder for Deep Learning of Images, Labels and Captions”, Neural Information Processing Systems (<strong>NeurIPS</strong>), 2016. 
<a style=" color: blue;" href="https://arxiv.org/pdf/1609.08976.pdf">PDF</a> /
<a style=" color: blue;" href="Papers/vae_nips2016_poster.pdf">Poster</a> 
</li>					

<li> Jiaming Song, <strong>Zhe Gan</strong> and Lawrence Carin “Factored Temporal Sigmoid Belief Networks for Sequence Learning”, Int. Conf. Machine Learning (<strong>ICML</strong>), 2016. 
<a style=" color: blue;" href="https://arxiv.org/pdf/1605.06715.pdf">PDF</a> /
<a style=" color: blue;" href="Papers/fctsbn_supp.zip">Supp</a> /
<a style=" color: blue;" href="Papers/fctsbn_poster.pdf">Poster</a> /
<a style=" color: blue;" href="Papers/fctsbn_slides.pdf">Slides</a>
</li>

<li> Chunyuan Li, Andrew Stevens, Changyou Chen, Yunchen Pu, <strong>Zhe Gan</strong> and Lawrence Carin "Learning Weight Uncertainty with Stochastic Gradient MCMC for Shape Classification", Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2016. 
<a style=" color: blue;" href="Papers/dbnn_shape_cvpr.pdf">PDF</a>  /
<a style=" color: blue;" href="Papers/dbnn_shape_cvpr_supp.pdf">Supp</a> /
<a style=" color: blue;" href="Papers/dbnn_shape_poster.pdf">Poster</a> /
<a style=" color: blue;" href="Papers/dbnn_shape_slides.pdf">Slides</a> 
(<font color="#FF0000"><strong>Spotlight</strong></font>)
</li>

<li> Changyou Chen, David Carlson, <strong>Zhe Gan</strong>, Chunyuan Li and Lawrence Carin “Bridging the Gap between Stochastic Gradient MCMC and Stochastic Optimization”, Artificial Intelligence and Statistics (<strong>AISTATS</strong>), 2016. 
<a style=" color: blue;" href="https://arxiv.org/pdf/1512.07962.pdf">PDF</a> /
<a style=" color: blue;" href="Papers/Santa_poster.pdf">Poster</a> /
<a style=" color: blue;" href="Papers/Santa_slides.pdf">Slides</a> / 
<a style=" color: #CC5500;" href="https://github.com/cchangyou/Santa">Code</a> 
(<font color="#FF0000"><strong>Oral</strong></font>) 
</li>

</ul>

<ul class="articlesubtitle" style="margin-top: -0.25in; margin-bottom: -0.01in;">
<strong>2015</strong>
</ul>
				
<ul class="articlesubtitle" style="margin-left: 0.2in;">
<li> Ricardo Henao, <strong>Zhe Gan</strong>, James Lu and Lawrence Carin "Deep Poisson Factor Modeling", Neural Information Processing Systems (<strong>NeurIPS</strong>), 2015.
<a style=" color: blue;" href="Papers/DPFM_NIPS2015.pdf">PDF</a> /
<a style=" color: blue;" href="Papers/DPFMsupp_NIPS2015.pdf">Supp</a> /
<a style=" color: blue;" href="Papers/DPFM_NIPS2015_poster.pdf">Poster</a> /
<a style=" color: #CC5500;" href="https://github.com/rhenaog/dpfm_nips2015">Code</a>
</li>

<li> <strong>Zhe Gan</strong>, Chunyuan Li, Ricardo Henao, David Carlson and Lawrence Carin "Deep Temporal Sigmoid Belief Networks for Sequence Modeling", Neural Information Processing Systems (<strong>NeurIPS</strong>), 2015.
<a style=" color: blue;" href="https://arxiv.org/pdf/1509.07087.pdf">PDF</a> /
<a style=" color: blue;" href="Papers/TSBN_NIPS2015_poster.pdf">Poster</a> /
<a style=" color: blue;" href="Papers/TSBN_slides.pdf">Slides</a> / 
<a style=" color: #CC5500;" href="https://github.com/zhegan27/TSBN_code_NIPS2015">Code</a>
</li>

<li> <strong>Zhe Gan</strong>, Changyou Chen, Ricardo Henao, David Carlson and Lawrence Carin "Scalable Deep Poisson Factor Analysis for Topic Modeling", Int. Conf. Machine Learning (<strong>ICML</strong>), 2015.
<a style=" color: blue;" href="Papers/DeepPFA_ICML2015.pdf">PDF</a> /
<a style=" color: blue;" href="Papers/DeepPFAsupp_ICML2015.pdf">Supp</a> /
<a style=" color: blue;" href="Papers/DeepPFA_ICML_poster.pdf">Poster</a> /
 <a style=" color: blue;" href="Papers/DeepPFA_ICML_slides.pdf">Slides</a> / 
 <a style=" color: #CC5500;" href="https://github.com/zhegan27/dpfa_icml2015">Code</a>
</li>

<li> <strong>Zhe Gan</strong>, Ricardo Henao, David Carlson and Lawrence Carin "Learning Deep Sigmoid Belief Networks with Data Augmentation", Artificial Intelligence and Statistics (<strong>AISTATS</strong>), 2015. 
<a style=" color: blue;" href="Papers/DeepSBN_AISTATS2015.pdf">PDF</a>  /
<a style=" color: blue;" href="Papers/DeepSBNsupp_AISTATS2015.pdf">Supp</a> /
<a style=" color: blue;" href="Papers/DeepSBN_AISTATS2015_poster.pdf">Poster</a> /
<a style=" color: #CC5500;" href="https://github.com/zhegan27/dsbn_aistats2015">Code</a>
</li>

<li> <strong>Zhe Gan</strong>, Xin Yuan, Ricardo Henao, Ephraim Tsalik and Lawrence Carin "Inference of Gene Networks Associated with the Host Response to Infectious Disease", Chapter 13 of the Book "Big Data Over Networks". Cambridge University Press. In Press.
<a style=" color: blue;" href="https://www.cambridge.org/core/books/big-data-over-networks/inference-of-gene-networks-associated-with-the-host-response-to-infectious-disease/2F67276A10DA40B51E751480A0FE8F6C">Link</a> /
<a style=" color: blue;" href="Papers/DFM_book_chapter.pdf">PDF</a>  /
<a style=" color: #CC5500;" href="https://www.dropbox.com/sh/eoffqtb093ox5yg/AABxEV7wgnjR9nq9lfmGgEtXa?dl=0">Code</a>
</li>

</ul>

<p>

<h2>Tutorial and Workshop </h2>
				
<ul class="articlesubtitle" style="margin-left: 0.2in;">
<li> Linjie Li, <strong>Zhe Gan</strong>, Chunyuan Li, Jianwei Yang and Zhengyuan Yang "Recent Advances in Vision Foundation Models", Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023. 
<a style=" color: blue;" href="https://vlp-tutorial.github.io/">Tutorial Website</a>
</li>

<li> <strong>Zhe Gan</strong>, Linjie Li, Chunyuan Li, Jianwei Yang, Pengchuan Zhang, Lijuan Wang, Zicheng Liu and Jianfeng Gao "Recent Advances in Vision-and-Language Pre-training", Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2022. 
<a style=" color: blue;" href="https://vlp-tutorial.github.io/2022/index.html">Tutorial Website</a>
</li>

<li> Man Luo, Tejas Gokhale, Zhiyuan Fang, Pratyay Banerjee, Yezhou Yang, Chitta Baral, Damien Teney, <strong>Zhe Gan</strong>, Kenneth Marino, TianLu Wang and Somak Aditya "O-DRUM: Workshop on Open-Domain Retrieval Under a Multi-Modal Setting", Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2022. 
<a style=" color: blue;" href="https://asu-apg.github.io/odrum/">Workshop Website</a>
</li>

<li> <strong>Zhe Gan</strong>, Chunyuan Li, Jianwei Yang and Pengchuan Zhang "Microsoft Vision+Language Summer Talk Series", 2021. 
<a style=" color: blue;" href="https://www.microsoft.com/en-us/research/videos/vision-language-summer-talk-series/">MSR Website</a> /
<a style=" color: blue;" href="https://www.youtube.com/playlist?list=PLD7HFcN7LXReRU4tDhN4VPTl4BpyTKAco">YouTube</a> /
<a style=" color: blue;" href="https://space.bilibili.com/110487933/channel/seriesdetail?sid=471333">Bilibili</a>
</li>

<li> Peter Anderson, Yoav Artzi, <strong>Zhe Gan</strong>, Xiaodong He, Linjie Li, Jingjing Liu, Xin (Eric) Wang, Qi Wu, and Luowei Zhou "From VQA to VLN: Recent Advances in Vision-and-Language Research", Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2021. 
<a style=" color: blue;" href="https://vqa2vln-tutorial.github.io/">Tutorial Website</a>
</li>

<li> <strong>Zhe Gan</strong>, Licheng Yu, Yu Cheng, Luowei Zhou, Linjie Li, Yen-Chun Chen,  Jingjing Liu and Xiaodong He "Recent Advances in Vision-and-Language Research", Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2020. 
<a style=" color: blue;" href="https://rohit497.github.io/Recent-Advances-in-Vision-and-Language-Research/">Tutorial Website</a>
</li>

<li> Peter Knees and <strong>Zhe Gan</strong> "The ACM Multimedia 2020
Interactive Arts Exhibition". 
<a style=" color: blue;" href="https://acmmm20-interactivearts.github.io/">Website</a>
</li>

</ul>

<p>

<p>

<h2>PhD Dissertation </h2>
				
<ul class="articlesubtitle" style="margin-left: 0.2in;">

<li> <strong>Zhe Gan</strong> "Deep Generative Models for Vision and Language Intelligence", Duke University.
<a style=" color: blue;" href="Papers/dissertation.pdf">PDF</a> 
</li>

</ul>


<p class="credits">&copy; January 2024 Zhe Gan<br />
		
		
	</div>

</body>
</html>
