<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 2.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
	<meta name="description" content="Research Scientist, Apple AI/ML" />
	<meta name="keywords" content="Zhe Gan, Deep Learning, Machine Learning, Natural Language Processing, Computer Vision, Apple, Microsoft, Duke University, Peking University" />
	<meta name="Zhe Gan" content="Research" />
	<link rel="stylesheet" type="text/css" href="Zhe.css" title="Basic Profile" media="all" />
	<title>Zhe Gan</title>
</head>

<body>


	<div id="sidebar">
		<a href="index.html"><img src="images/Zhe_new2.jpeg"  height="180" alt="Sample logotype" /></a>
		<h1><a style="text-decoration: none" href="index.html">Zhe Gan</a></h1>
		<!-- <p class="slogan">everyone has a story to tell</p> -->
		
		<ul>
			<!--<li><a href="#">Page one</a><br />The front page...</li>-->
			<li><a href="Paper.html">Publications</a></li>
			<li><a href="https://scholar.google.com/citations?user=E64XWyMAAAAJ&hl=en">Google Scholar</a></li>
			<li><a href="https://github.com/zhegan27">GitHub</a></li>
			<li><a href="https://www.linkedin.com/pub/zhe-gan/78/29a/a22">LinkedIn</a></li>
			<li><a href="CV_Zhe.pdf">CV</a></li>
			<p>
			<font size="2">Research Scientist and Manager<br>
			Apple AI/ML<br>
			Seattle, WA 98109<br>
			Email: zhe.gan@apple.com <br></font>
		</ul>
	</div>
	
	<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-35321140-3', ‘zhegan.github.io');
  ga('send', 'pageview');

</script>
	
	<!--style="font-family: Calibri; color: blue; text-decoration: underline;"-->

	<div id="content">
		<!--<h2>What Starts Here Changes The World!</h2>-->
		<p>
		<p>
		<br>
		
		<p> I am a Research Scientist and Manager at Apple AI/ML, primarily working on building large-scale vision and multimodal foundation models. Before joining Apple, I was a Principal Researcher at Microsoft Azure AI, working on <a style="color: #CC5500;" href="https://www.microsoft.com/en-us/research/project/project-florence-vl/">Project Florence-VL</a>. I received my Ph.D. degree from <a style="color: #CC5500;" href="http://www.duke.edu/">Duke University</a> in Spring 2018, and my Master's and B.Sc. degree from <a style="color: #CC5500;" href="http://www.pku.edu.cn/">Peking University</a> in 2013 and 2010, respectively. My Ph.D. advisor is <a style="color: #CC5500;" href="http://people.ee.duke.edu/~lcarin">Lawrence Carin</a>. I can be reached at pkuganzhe@gmail.com and zhe.gan@apple.com.


		<p> I am serving (or, has served) as a Senior Area Chair (SAC) for <a style="color: #CC5500;" href="https://2025.aclweb.org/">ACL 2025</a>, <a style="color: #CC5500;" href="https://2024.emnlp.org/">EMNLP 2024</a>, an Area Chair for <a style="color: #CC5500;" href="https://nips.cc/Conferences/2024">NeurIPS 2024/2023/2022/2021/2020/2019</a>, <a style="color: #CC5500;" href="https://icml.cc/Conferences/2024">ICML 2024/2023/2022/2021</a>, <a style="color: #CC5500;" href="https://iclr.cc/">ICLR 2024/2023/2021</a>, <a style="color: #CC5500;" href="https://cvpr2024.thecvf.com/">CVPR 2024/2023</a>, <a style="color: #CC5500;" href="https://eccv2022.ecva.net/">ECCV 2022</a>, <a style="color: #CC5500;" href="https://wacv2024.thecvf.com/">WACV 2024</a>, <a style="color: #CC5500;" href="https://2024.aclweb.org/">ACL 2024/2022/2021</a>, <a style="color: #CC5500;" href="https://2024.naacl.org/">NAACL 2024/2022</a>, <a style="color: #CC5500;" href="https://2023.emnlp.org/">EMNLP 2023/2022</a>, <a style="color: #CC5500;" href="https://colmweb.org/">COLM 2024</a>,  <a style="color: #CC5500;" href="https://aaai.org/Conferences/AAAI/aaai.php">AAAI 2023/2022</a>, and a Senior Program Committee (SPC) member for <a style="color: #CC5500;" href="https://aaai.org/Conferences/AAAI/aaai.php">AAAI 2021/2020</a>, and received <a style="color: #CC5500;" href="https://aaai.org/Awards/conference.php">AAAI-20 Outstanding SPC Award</a>. Together with my co-authors, I have also been honored with the Best Student Paper Honorable Mention Awards at CVPR 2021 and WACV 2021, respectively.
		
		<p>
		<br/>
		
		<h3>Research Highlights:  </h3>
		<ul>

		<p> 
		<li>
		[2024/7] <a style="color: #CC5500;" href="https://arxiv.org/pdf/2404.07973.pdf">Ferret-v2</a> got accepted to COLM 2024.
		</li>

		<p> 
		<li>
		[2024/7] 4 papers accepted to ECCV 2024: <a style="color: #CC5500;" href="https://arxiv.org/abs/2403.09611">MM1</a>, <a style="color: #CC5500;" href="https://arxiv.org/pdf/2404.05719">Ferret-UI</a>, <a style="color: #CC5500;" href="https://arxiv.org/pdf/2310.07699">VeCLIP</a>, and <a style="color: #CC5500;" href="https://arxiv.org/pdf/2212.00280">GRiT</a>. Also, please checkout my talk on MM1 at CVPR 2024 <a style="color: #CC5500;" href="https://vlp-tutorial.github.io/">tutorial</a> and <a style="color: #CC5500;" href="https://computer-vision-in-the-wild.github.io/cvpr-2024/">workshop</a> (<a style="color: #CC5500;" href="https://datarelease.blob.core.windows.net/tutorial/vision_foundation_models_2024/Zhe_pretraining.pdf">slides</a>).
		</li>

		<p> 
		<li>
		[2024/3] Please checkout our new paper <a style="color: #CC5500;" href="https://arxiv.org/abs/2403.09611">MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training</a>.
		</li>

		<p> 
		<li>
		[2024/2] 4 papers accepted to ICLR 2024: (1) Ferret: a new multimodal LLM that can refer and ground anything anywhere at any granularity (<a style="color: #CC5500;" href="https://arxiv.org/pdf/2310.07704.pdf">here</a>), (2) MGIE: multimodal LLM for guiding instruction-based image editing (<a style="color: #CC5500;" href="https://arxiv.org/pdf/2309.17102.pdf">here</a>), (3) compressing LLMs: the truth is rarely pure and never simple (<a style="color: #CC5500;" href="https://arxiv.org/pdf/2310.01382.pdf">here</a>), and (4) MOFI: learning image representations from noisy entity annotated images (<a style="color: #CC5500;" href="https://arxiv.org/pdf/2306.07952.pdf">here</a>).
		</li>

		<p> 
		<li>
		[2023/9] Please checkout our survey paper/book on  
		<a style="color: #CC5500;" href="https://arxiv.org/pdf/2309.10020.pdf">Multimodal Foundation Models: From Specialists to General-Purpose Assistants</a>. 
		</li>

		<p> 
		<li>
		[2023/6] We held a tutorial on Recent Advances in Vision Foundation Models at CVPR 2023. All the slides can now be downloaded from the 
		<a style="color: #CC5500;" href="https://vlp-tutorial.github.io/">tutorial webpage</a>. 
		</li>

		<p> 
		<li>
		[2023/6] <a style="color: #CC5500;" href="https://arxiv.org/abs/2306.07952">MOFI</a> is our new vision foundation model that is designed to learn image representations from noisy entity annotated images. To achieve this, we have created Image-to-Entities (I2E), a new large-scale dataset with 1 billion images and 2 million distinct entities, covering rich visual concepts in the wild.
		</li>

		<p> 
		<li>
		[2023/2] 5 papers accepted to CVPR 2023: (1) X-decoder: generalist modeling for (open-vocab) segmentation and vision-language tasks; (2) ReCo: region-controlled text-to-image generation; (3) x-CLIP: enhancing CLIP with non-contrastive learning; (4) LAVENDER and VIOLET-v2: two empirical studies on video-language pre-training.
		</li>

		<p> 
		<li>
		[2023/1] Our recent work on <a style="color: #CC5500;" href="https://arxiv.org/pdf/2210.09150.pdf">Prompting GPT-3 To Be Reliable</a> got accepted to ICLR 2023.
		</li>

		<p> 
		<li>
		[2023/1] Gave an invited talk on multimodal foundation models at <a style="color: #CC5500;" href="https://asu-apg.github.io/serum/">WACV 2023</a> [<a style="color: #CC5500;" href="https://www.dropbox.com/s/8utdqq2jhqdz5kt/WACVTalk.2023.1.7.pdf?dl=0">slides</a>].
		</li>

		<p> 
		<li>
		[2022/12] Please check out our new survey paper/book on <a style="color: #CC5500;" href="https://arxiv.org/pdf/2210.09263.pdf">Vision-Language Pre-training: Basics, Recent Advances, and Future Trends</a>, published at <a style="color: #CC5500;" href="https://www.nowpublishers.com/article/Details/CGV-105">Foundations and Trends in Computer Graphics and Vision</a>.
		</li>

		<p> 
		<li>
		[2022/9] 3 papers accepted to NeurIPS 2022: (1) <a style="color: #CC5500;" href="https://arxiv.org/abs/2207.09814">NUWA Infinity</a>, our new multimodal generative model for image synthesis; (2) <a style="color: #CC5500;" href="https://arxiv.org/pdf/2206.07643.pdf">FIBER</a>, our new VLP model that provides a unified solution for both VL understanding and localization tasks; and (3) <a style="color: #CC5500;" href="https://arxiv.org/pdf/2204.09222.pdf">K-Lite</a>, which explores how to enhance UniCL and GLIP models with external knowledge.  
		</li>

		<p> 
		<li>
		[2022/9] <a style="color: #CC5500;" href="https://arxiv.org/pdf/2111.12085.pdf">UniTAB</a> is accepted as an Oral paper at ECCV 2022. Following the line of work such as Pix2seq, Pix2seqV2, OFA, and Unified-IO, we propose a simple unified seq2seq learning framework that can output sequences with mixed text and box tokens.
		</li>

		<p> 
		<li>
		[2022/7] <a style="color: #CC5500;" href="https://arxiv.org/abs/2207.09814">NUWA Infinity</a> is our new multimodal generative model that is able to generate high-quality images and videos from given text or image input. We can generate images with resolution up to 38912 × 2048 pixels. Check our project website <a style="color: #CC5500;" href="https://nuwa-infinity.microsoft.com">here</a>.
		</li>

		<p> 
		<li>
		[2022/6] We held a tutorial on recent advances on vision-language pre-training at CVPR 2022. All our slides are available at our <a style="color: #CC5500;" href="https://vlp-tutorial.github.io/">tutorial website</a> now.
		</li>

		<p> 
		<li>
		[2022/6] <a style="color: #CC5500;" href="https://arxiv.org/pdf/2205.14100.pdf">Florence-GIT</a> is our new multimodal generative foundation model, where we have trained a simple image-to-text transformer on 800M image-text pairs. GIT achieves new sota across 12 image/video captioning and QA tasks, including the first human-parity on TextCaps. GIT achieves an accuracy of 88.79% on ImageNet-1k using a generative scheme. GIT can recognize logos, landmarks, characters, etc.
		</li>


		<p class="credits">&copy; July 2024 Zhe Gan<br />
		</ul>
	</div>
</body>
</html>
